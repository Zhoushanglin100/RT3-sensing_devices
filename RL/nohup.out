2021-09-28 15:32:11.460228: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-28 15:32:11.508679: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2021-09-28 15:32:11.514428: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a4bf5205a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-09-28 15:32:11.514509: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-63
OMP: Info #156: KMP_AFFINITY: 64 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 2 packages x 16 cores/pkg x 2 threads/core (32 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 32 maps to package 0 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 33 maps to package 0 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 34 maps to package 0 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 35 maps to package 0 core 3 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 4 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 36 maps to package 0 core 4 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 5 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 37 maps to package 0 core 5 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 6 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 38 maps to package 0 core 6 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 7 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 39 maps to package 0 core 7 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 0 core 8 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 40 maps to package 0 core 8 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 0 core 9 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 41 maps to package 0 core 9 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 0 core 10 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 42 maps to package 0 core 10 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 0 core 11 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 43 maps to package 0 core 11 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 0 core 12 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 44 maps to package 0 core 12 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 0 core 13 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 45 maps to package 0 core 13 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 0 core 14 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 46 maps to package 0 core 14 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 0 core 15 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 47 maps to package 0 core 15 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 16 maps to package 1 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 48 maps to package 1 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 17 maps to package 1 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 49 maps to package 1 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 18 maps to package 1 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 50 maps to package 1 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 19 maps to package 1 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 51 maps to package 1 core 3 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 20 maps to package 1 core 4 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 52 maps to package 1 core 4 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 21 maps to package 1 core 5 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 53 maps to package 1 core 5 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 22 maps to package 1 core 6 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 54 maps to package 1 core 6 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 23 maps to package 1 core 7 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 55 maps to package 1 core 7 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 24 maps to package 1 core 8 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 56 maps to package 1 core 8 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 25 maps to package 1 core 9 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 57 maps to package 1 core 9 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 26 maps to package 1 core 10 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 58 maps to package 1 core 10 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 27 maps to package 1 core 11 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 59 maps to package 1 core 11 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 28 maps to package 1 core 12 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 60 maps to package 1 core 12 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 29 maps to package 1 core 13 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 61 maps to package 1 core 13 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 30 maps to package 1 core 14 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 62 maps to package 1 core 14 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 31 maps to package 1 core 15 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 63 maps to package 1 core 15 thread 1 
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 51900 thread 0 bound to OS proc set 0
2021-09-28 15:32:11.522956: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
/home/shz15022/anaconda3/envs/RT3/lib/python3.6/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1631630806732/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
/home/shz15022/RT3_opensource/RL/utils/load_config_file.py:11: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  raw_dict = yaml.load(stream)
/home/shz15022/anaconda3/envs/RT3/lib/python3.6/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1631630806732/work/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

#########################################################################################
A.pattern pruning from precompression model
B.extract important pattern from precompression model
C.training(pruning number=[10, 30, 50, 70, 80, 90])
#########################################################################################
2021-09-28 15:32:12,254 tensorflow   WARNING  
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From mnist_rl_controller.py:126: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
2021-09-28 15:32:12,982 tensorflow   WARNING  From mnist_rl_controller.py:126: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /home/shz15022/anaconda3/envs/RT3/lib/python3.6/site-packages/tensorflow_core/contrib/rnn/python/ops/rnn_cell.py:1529: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
2021-09-28 15:32:13,025 tensorflow   WARNING  From /home/shz15022/anaconda3/envs/RT3/lib/python3.6/site-packages/tensorflow_core/contrib/rnn/python/ops/rnn_cell.py:1529: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
WARNING:tensorflow:From mnist_rl_controller.py:131: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
2021-09-28 15:32:13,066 tensorflow   WARNING  From mnist_rl_controller.py:131: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /home/shz15022/anaconda3/envs/RT3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
2021-09-28 15:32:13,067 tensorflow   WARNING  From /home/shz15022/anaconda3/envs/RT3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/shz15022/anaconda3/envs/RT3/lib/python3.6/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2021-09-28 15:32:16,335 tensorflow   WARNING  From /home/shz15022/anaconda3/envs/RT3/lib/python3.6/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52528 thread 1 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52527 thread 2 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52756 thread 3 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52757 thread 4 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52758 thread 5 bound to OS proc set 5
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52759 thread 6 bound to OS proc set 6
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52760 thread 7 bound to OS proc set 7
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52761 thread 8 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52762 thread 9 bound to OS proc set 9
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52763 thread 10 bound to OS proc set 10
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52764 thread 11 bound to OS proc set 11
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52765 thread 12 bound to OS proc set 12
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52766 thread 13 bound to OS proc set 13
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52767 thread 14 bound to OS proc set 14
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52768 thread 15 bound to OS proc set 15
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52769 thread 16 bound to OS proc set 16
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52770 thread 17 bound to OS proc set 17
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52771 thread 18 bound to OS proc set 18
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52772 thread 19 bound to OS proc set 19
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52773 thread 20 bound to OS proc set 20
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52774 thread 21 bound to OS proc set 21
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52775 thread 22 bound to OS proc set 22
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52776 thread 23 bound to OS proc set 23
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52777 thread 24 bound to OS proc set 24
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52778 thread 25 bound to OS proc set 25
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52779 thread 26 bound to OS proc set 26
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52780 thread 27 bound to OS proc set 27
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52781 thread 28 bound to OS proc set 28
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52782 thread 29 bound to OS proc set 29
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52783 thread 30 bound to OS proc set 30
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52785 thread 32 bound to OS proc set 32
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52784 thread 31 bound to OS proc set 31
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52786 thread 33 bound to OS proc set 33
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52787 thread 34 bound to OS proc set 34
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52788 thread 35 bound to OS proc set 35
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52789 thread 36 bound to OS proc set 36
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52790 thread 37 bound to OS proc set 37
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52791 thread 38 bound to OS proc set 38
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52792 thread 39 bound to OS proc set 39
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52793 thread 40 bound to OS proc set 40
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52794 thread 41 bound to OS proc set 41
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52795 thread 42 bound to OS proc set 42
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52796 thread 43 bound to OS proc set 43
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52797 thread 44 bound to OS proc set 44
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52798 thread 45 bound to OS proc set 45
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52799 thread 46 bound to OS proc set 46
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52800 thread 47 bound to OS proc set 47
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52801 thread 48 bound to OS proc set 48
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52802 thread 49 bound to OS proc set 49
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52803 thread 50 bound to OS proc set 50
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52804 thread 51 bound to OS proc set 51
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52805 thread 52 bound to OS proc set 52
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52806 thread 53 bound to OS proc set 53
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52807 thread 54 bound to OS proc set 54
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52808 thread 55 bound to OS proc set 55
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52809 thread 56 bound to OS proc set 56
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52810 thread 57 bound to OS proc set 57
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52811 thread 58 bound to OS proc set 58
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52812 thread 59 bound to OS proc set 59
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52813 thread 60 bound to OS proc set 60
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52814 thread 61 bound to OS proc set 61
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52815 thread 62 bound to OS proc set 62
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52817 thread 64 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52816 thread 63 bound to OS proc set 63
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52818 thread 65 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52819 thread 66 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52820 thread 67 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52821 thread 68 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52822 thread 69 bound to OS proc set 5
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52823 thread 70 bound to OS proc set 6
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52824 thread 71 bound to OS proc set 7
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52825 thread 72 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52826 thread 73 bound to OS proc set 9
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52827 thread 74 bound to OS proc set 10
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52828 thread 75 bound to OS proc set 11
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52829 thread 76 bound to OS proc set 12
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52830 thread 77 bound to OS proc set 13
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52831 thread 78 bound to OS proc set 14
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52832 thread 79 bound to OS proc set 15
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52833 thread 80 bound to OS proc set 16
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52834 thread 81 bound to OS proc set 17
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52835 thread 82 bound to OS proc set 18
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52836 thread 83 bound to OS proc set 19
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52838 thread 85 bound to OS proc set 21
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52837 thread 84 bound to OS proc set 20
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52839 thread 86 bound to OS proc set 22
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52840 thread 87 bound to OS proc set 23
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52841 thread 88 bound to OS proc set 24
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52842 thread 89 bound to OS proc set 25
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52843 thread 90 bound to OS proc set 26
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52844 thread 91 bound to OS proc set 27
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52845 thread 92 bound to OS proc set 28
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52846 thread 93 bound to OS proc set 29
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52847 thread 94 bound to OS proc set 30
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52848 thread 95 bound to OS proc set 31
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52849 thread 96 bound to OS proc set 32
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52850 thread 97 bound to OS proc set 33
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52851 thread 98 bound to OS proc set 34
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52852 thread 99 bound to OS proc set 35
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52853 thread 100 bound to OS proc set 36
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52854 thread 101 bound to OS proc set 37
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52855 thread 102 bound to OS proc set 38
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52856 thread 103 bound to OS proc set 39
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52857 thread 104 bound to OS proc set 40
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52858 thread 105 bound to OS proc set 41
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52859 thread 106 bound to OS proc set 42
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52860 thread 107 bound to OS proc set 43
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52861 thread 108 bound to OS proc set 44
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52862 thread 109 bound to OS proc set 45
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52863 thread 110 bound to OS proc set 46
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52864 thread 111 bound to OS proc set 47
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52865 thread 112 bound to OS proc set 48
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52866 thread 113 bound to OS proc set 49
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52867 thread 114 bound to OS proc set 50
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52869 thread 116 bound to OS proc set 52
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52870 thread 117 bound to OS proc set 53
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52871 thread 118 bound to OS proc set 54
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52872 thread 119 bound to OS proc set 55
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52873 thread 120 bound to OS proc set 56
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52874 thread 121 bound to OS proc set 57
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52876 thread 123 bound to OS proc set 59
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52875 thread 122 bound to OS proc set 58
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52868 thread 115 bound to OS proc set 51
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52877 thread 124 bound to OS proc set 60
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52878 thread 125 bound to OS proc set 61
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52879 thread 126 bound to OS proc set 62
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52880 thread 127 bound to OS proc set 63
OMP: Info #250: KMP_AFFINITY: pid 51900 tid 52881 thread 128 bound to OS proc set 0
[at weight fc1.weight]
percentage of pruned: 0.0000%
nonzero parameters after pruning: 30720 / 30720

[at weight fc2.weight]
percentage of pruned: 0.0000%
nonzero parameters after pruning: 10080 / 10080

[at weight fc3.weight]
percentage of pruned: 0.0000%
nonzero parameters after pruning: 840 / 840

total pruning rate: 44426 / 44426 (0.0000%)
-----------------------------------------------------------------------------------------
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 0<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 30.0879%
nonzero parameters after pruning: 21477 / 30720

[at weight fc2.weight]
percentage of pruned: 29.7321%
nonzero parameters after pruning: 7083 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31935 / 44426 (28.1164%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 49.9870%
nonzero parameters after pruning: 15364 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9603%
nonzero parameters after pruning: 5044 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23618 / 44426 (46.8374%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.8503%
nonzero parameters after pruning: 9262 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7222%
nonzero parameters after pruning: 3052 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15353 / 44426 (65.4414%)
-----------------------------------------------------------------------------------------
latency_list: [18.012830464285713, 17.9958957, 13.5237286875]
runs_number: [4874257.0, 5542701.0, 7093724.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9368
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6584
| test_data evaluate | sub loss  0.01 | sub accuracy   0.4019
| test before training | weighted_accuracy   0.7463
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.27 ms/batch  | loss  0.16
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9794
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9791
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9669
| end of epoch   1 | time: 22.02s | weighted_accuracy   0.9768
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9794
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9791
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9669
| end of training | weighted_accuracy   0.9768
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 0,sparsity_level:[1, 2, 3]
--------->Accuracy reward: 0.8534442053900825,time reward:6.913946363636363
--------->Reward: 7.767390569026445
==========================================================================================
=-=-=-=-=-=>Episode: 0 | Loss: [1.6141039] | LR: (0.99, 1) | Mean R: 7.767390569026445 | Reward: [0.0]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 1<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9609%
nonzero parameters after pruning: 27660 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9306%
nonzero parameters after pruning: 9079 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40281 / 44426 (9.3301%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.8810%
nonzero parameters after pruning: 421 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0521%
nonzero parameters after pruning: 6128 / 30720

[at weight fc2.weight]
percentage of pruned: 79.7024%
nonzero parameters after pruning: 2046 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11127 / 44426 (74.9539%)
-----------------------------------------------------------------------------------------
latency_list: [23.18943696428571, 17.98981725, 8.9366698125]
runs_number: [3786170.0, 5544574.0, 10734827.0]
====================Results=======================
--------->Episode: 1,sparsity_level:[0, 2, 4]
--------->Accuracy reward: -1,time reward:8.075259545454545
--------->Reward: 7.075259545454545
==========================================================================================
=-=-=-=-=-=>Episode: 1 | Loss: [1.6152577] | LR: (0.99, 2) | Mean R: 7.075259545454545 | Reward: [-0.5059883223182045]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 2<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 10.0651%
nonzero parameters after pruning: 27628 / 30720

[at weight fc2.weight]
percentage of pruned: 10.1488%
nonzero parameters after pruning: 9057 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40227 / 44426 (9.4517%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 49.9870%
nonzero parameters after pruning: 15364 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23631 / 44426 (46.8082%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.8535%
nonzero parameters after pruning: 9261 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6429%
nonzero parameters after pruning: 3060 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15361 / 44426 (65.4234%)
-----------------------------------------------------------------------------------------
latency_list: [23.155943464285713, 18.00718425, 13.5324121875]
runs_number: [3791647.0, 5539226.0, 7089172.0]
====================Results=======================
--------->Episode: 2,sparsity_level:[0, 2, 3]
--------->Accuracy reward: -1,time reward:6.418202272727273
--------->Reward: 5.418202272727273
==========================================================================================
=-=-=-=-=-=>Episode: 2 | Loss: [1.6178353] | LR: (0.99, 3) | Mean R: 5.418202272727273 | Reward: [-1.698858983554703]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 3<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 30.0651%
nonzero parameters after pruning: 21484 / 30720

[at weight fc2.weight]
percentage of pruned: 29.7222%
nonzero parameters after pruning: 7084 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31943 / 44426 (28.0984%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 49.9870%
nonzero parameters after pruning: 15364 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8115%
nonzero parameters after pruning: 5059 / 10080

[at weight fc3.weight]
percentage of pruned: 49.8810%
nonzero parameters after pruning: 421 / 840

total pruning rate: 23630 / 44426 (46.8104%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9414%
nonzero parameters after pruning: 6162 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11162 / 44426 (74.8751%)
-----------------------------------------------------------------------------------------
latency_list: [18.017792464285712, 18.0063159, 8.974660124999998]
runs_number: [4872914.0, 5539494.0, 10689386.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8960
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6534
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1676
| test before training | weighted_accuracy   0.6775
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.27 ms/batch  | loss  0.21
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9782
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9741
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9611
| end of epoch   1 | time: 22.05s | weighted_accuracy   0.9735
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9782
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9741
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9611
| end of training | weighted_accuracy   0.9735
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 3,sparsity_level:[1, 2, 4]
--------->Accuracy reward: 0.8172218004862468,time reward:8.54627
--------->Reward: 9.363491800486246
==========================================================================================
=-=-=-=-=-=>Episode: 3 | Loss: [1.6193805] | LR: (0.99, 4) | Mean R: 9.363491800486246 | Reward: [2.0252575354698372]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 4<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9609%
nonzero parameters after pruning: 27660 / 30720

[at weight fc2.weight]
percentage of pruned: 9.8810%
nonzero parameters after pruning: 9084 / 10080

[at weight fc3.weight]
percentage of pruned: 9.8810%
nonzero parameters after pruning: 757 / 840

total pruning rate: 40287 / 44426 (9.3166%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7222%
nonzero parameters after pruning: 3052 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15327 / 44426 (65.4999%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9805%
nonzero parameters after pruning: 6150 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6429%
nonzero parameters after pruning: 2052 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11155 / 44426 (74.8908%)
-----------------------------------------------------------------------------------------
latency_list: [23.193158464285712, 10.79640585, 8.967062062500002]
runs_number: [3785563.0, 9238804.0, 10698443.0]
====================Results=======================
--------->Episode: 4,sparsity_level:[0, 3, 4]
--------->Accuracy reward: -1,time reward:9.737640909090908
--------->Reward: 8.737640909090908
==========================================================================================
=-=-=-=-=-=>Episode: 4 | Loss: [1.6211071] | LR: (0.99, 5) | Mean R: 8.737640909090908 | Reward: [1.23470126012462]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 5<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 29.9740%
nonzero parameters after pruning: 21512 / 30720

[at weight fc2.weight]
percentage of pruned: 29.8512%
nonzero parameters after pruning: 7071 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31958 / 44426 (28.0646%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.8535%
nonzero parameters after pruning: 9261 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15355 / 44426 (65.4369%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9805%
nonzero parameters after pruning: 6150 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11150 / 44426 (74.9021%)
-----------------------------------------------------------------------------------------
latency_list: [18.027096214285713, 10.820719650000001, 8.961634875]
runs_number: [4870399.0, 9218044.0, 10704922.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9392
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4101
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1857
| test before training | weighted_accuracy   0.6298
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.99 ms/batch  | loss  0.22
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9775
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9699
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9629
| end of epoch   1 | time: 21.82s | weighted_accuracy   0.9723
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9775
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9699
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9629
| end of training | weighted_accuracy   0.9723
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 5,sparsity_level:[1, 3, 4]
--------->Accuracy reward: 0.803332593705919,time reward:10.224256818181818
--------->Reward: 11.027589411887737
==========================================================================================
=-=-=-=-=-=>Episode: 5 | Loss: [1.6155276] | LR: (0.99, 6) | Mean R: 11.027589411887737 | Reward: [3.164246345205668]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 6<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9609%
nonzero parameters after pruning: 27660 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9504%
nonzero parameters after pruning: 9077 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40279 / 44426 (9.3346%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8115%
nonzero parameters after pruning: 5059 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23613 / 44426 (46.8487%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.8535%
nonzero parameters after pruning: 9261 / 30720

[at weight fc2.weight]
percentage of pruned: 69.9603%
nonzero parameters after pruning: 3028 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15329 / 44426 (65.4954%)
-----------------------------------------------------------------------------------------
latency_list: [23.188196464285713, 17.991553950000004, 13.497678187500002]
runs_number: [3786373.0, 5544039.0, 7107415.0]
====================Results=======================
--------->Episode: 6,sparsity_level:[0, 2, 3]
--------->Accuracy reward: -1,time reward:6.426285
--------->Reward: 5.426285
==========================================================================================
=-=-=-=-=-=>Episode: 6 | Loss: [1.6081839] | LR: (0.99, 7) | Mean R: 5.426285 | Reward: [-2.285975218190769]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 7<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0391%
nonzero parameters after pruning: 15348 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9008%
nonzero parameters after pruning: 5050 / 10080

[at weight fc3.weight]
percentage of pruned: 49.7619%
nonzero parameters after pruning: 422 / 840

total pruning rate: 23606 / 44426 (46.8644%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.0944%
nonzero parameters after pruning: 9187 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6429%
nonzero parameters after pruning: 3060 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15286 / 44426 (65.5922%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9805%
nonzero parameters after pruning: 6150 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11150 / 44426 (74.9021%)
-----------------------------------------------------------------------------------------
latency_list: [12.846768214285714, 10.7608035, 8.961634875]
runs_number: [6834338.0, 9269371.0, 10704922.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7345
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4440
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1521
| test before training | weighted_accuracy   0.5309
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.21 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9735
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9734
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9644
| end of epoch   1 | time: 22.15s | weighted_accuracy   0.9717
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9735
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9734
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9644
| end of training | weighted_accuracy   0.9717
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 7,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7961111598544651,time reward:11.140286818181817
--------->Reward: 11.936397978036283
==========================================================================================
=-=-=-=-=-=>Episode: 7 | Loss: [1.6060307] | LR: (0.99, 8) | Mean R: 11.936397978036283 | Reward: [3.9034800084090797]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 8<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9707%
nonzero parameters after pruning: 27657 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9107%
nonzero parameters after pruning: 9081 / 10080

[at weight fc3.weight]
percentage of pruned: 9.8810%
nonzero parameters after pruning: 757 / 840

total pruning rate: 40281 / 44426 (9.3301%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.7619%
nonzero parameters after pruning: 422 / 840

total pruning rate: 23623 / 44426 (46.8262%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.0879%
nonzero parameters after pruning: 9189 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7024%
nonzero parameters after pruning: 3054 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15283 / 44426 (65.5990%)
-----------------------------------------------------------------------------------------
latency_list: [23.18943696428571, 18.00023745, 13.4477480625]
runs_number: [3786170.0, 5541364.0, 7133804.0]
====================Results=======================
--------->Episode: 8,sparsity_level:[0, 2, 3]
--------->Accuracy reward: -1,time reward:6.436971818181818
--------->Reward: 5.436971818181818
==========================================================================================
=-=-=-=-=-=>Episode: 8 | Loss: [1.6178107] | LR: (0.99, 9) | Mean R: 5.436971818181818 | Reward: [-2.4635035387717963]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 9<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0130%
nonzero parameters after pruning: 15356 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9008%
nonzero parameters after pruning: 5050 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23616 / 44426 (46.8419%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1042%
nonzero parameters after pruning: 9184 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15279 / 44426 (65.6080%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9544%
nonzero parameters after pruning: 6158 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11158 / 44426 (74.8841%)
-----------------------------------------------------------------------------------------
latency_list: [12.852970714285714, 10.754725050000001, 8.970318374999998]
runs_number: [6831040.0, 9274609.0, 10694559.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7722
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4223
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2199
| test before training | weighted_accuracy   0.5568
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.78 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9787
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9727
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9682
| end of epoch   1 | time: 21.66s | weighted_accuracy   0.9748
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9787
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9727
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9682
| end of training | weighted_accuracy   0.9748
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 9,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8311110072665745,time reward:11.136458181818181
--------->Reward: 11.967569189084756
==========================================================================================
=-=-=-=-=-=>Episode: 9 | Loss: [1.6203189] | LR: (0.99, 10) | Mean R: 11.967569189084756 | Reward: [3.8182360000389526]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 10<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 30.0846%
nonzero parameters after pruning: 21478 / 30720

[at weight fc2.weight]
percentage of pruned: 29.7817%
nonzero parameters after pruning: 7078 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31931 / 44426 (28.1254%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.0879%
nonzero parameters after pruning: 9189 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 3048 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15277 / 44426 (65.6125%)
-----------------------------------------------------------------------------------------
latency_list: [18.010349464285714, 17.9906856, 13.441235437499998]
runs_number: [4874928.0, 5544306.0, 7137261.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8666
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7668
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4659
| test before training | weighted_accuracy   0.7565
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.80 ms/batch  | loss  0.14
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9803
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9716
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9657
| end of epoch   1 | time: 21.68s | weighted_accuracy   0.9748
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9803
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9716
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9657
| end of training | weighted_accuracy   0.9748
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 10,sparsity_level:[1, 2, 3]
--------->Accuracy reward: 0.8307772212558323,time reward:6.934770454545454
--------->Reward: 7.765547675801287
==========================================================================================
=-=-=-=-=-=>Episode: 10 | Loss: [1.6124113] | LR: (0.99, 11) | Mean R: 7.765547675801287 | Reward: [-0.3878252450167068]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 11<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9609%
nonzero parameters after pruning: 27660 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9306%
nonzero parameters after pruning: 9079 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40281 / 44426 (9.3301%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 30.0814%
nonzero parameters after pruning: 21479 / 30720

[at weight fc2.weight]
percentage of pruned: 29.7421%
nonzero parameters after pruning: 7082 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31936 / 44426 (28.1142%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9603%
nonzero parameters after pruning: 5044 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
latency_list: [23.18943696428571, 25.218831, 22.488357]
runs_number: [3786170.0, 3955214.0, 4265923.0]
====================Results=======================
--------->Episode: 11,sparsity_level:[0, 1, 2]
--------->Accuracy reward: -1,time reward:4.4124122727272725
--------->Reward: 3.4124122727272725
==========================================================================================
=-=-=-=-=-=>Episode: 11 | Loss: [1.6205051] | LR: (0.99, 12) | Mean R: 3.4124122727272725 | Reward: [-4.530591736470472]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 12<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 10.0749%
nonzero parameters after pruning: 27625 / 30720

[at weight fc2.weight]
percentage of pruned: 10.1786%
nonzero parameters after pruning: 9054 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40221 / 44426 (9.4652%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.0879%
nonzero parameters after pruning: 9189 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7421%
nonzero parameters after pruning: 3050 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15278 / 44426 (65.6102%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0521%
nonzero parameters after pruning: 6128 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11128 / 44426 (74.9516%)
-----------------------------------------------------------------------------------------
latency_list: [23.152221964285715, 10.7538567, 8.937755249999999]
runs_number: [3792256.0, 9275358.0, 10733523.0]
====================Results=======================
--------->Episode: 12,sparsity_level:[0, 3, 4]
--------->Accuracy reward: -1,time reward:9.77324409090909
--------->Reward: 8.77324409090909
==========================================================================================
=-=-=-=-=-=>Episode: 12 | Loss: [1.6361598] | LR: (0.99, 13) | Mean R: 8.77324409090909 | Reward: [0.7902177924556408]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 13<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 10.0651%
nonzero parameters after pruning: 27628 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9405%
nonzero parameters after pruning: 9078 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40248 / 44426 (9.4044%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 30.0130%
nonzero parameters after pruning: 21500 / 30720

[at weight fc2.weight]
percentage of pruned: 29.7619%
nonzero parameters after pruning: 7080 / 10080

[at weight fc3.weight]
percentage of pruned: 30.0000%
nonzero parameters after pruning: 588 / 840

total pruning rate: 31954 / 44426 (28.0737%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.0879%
nonzero parameters after pruning: 9189 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6825%
nonzero parameters after pruning: 3056 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15285 / 44426 (65.5945%)
-----------------------------------------------------------------------------------------
latency_list: [23.16896871428571, 25.234461300000003, 13.449918937499998]
runs_number: [3789515.0, 3952764.0, 7132653.0]
====================Results=======================
--------->Episode: 13,sparsity_level:[0, 1, 3]
--------->Accuracy reward: -1,time reward:5.715878181818182
--------->Reward: 4.715878181818182
==========================================================================================
=-=-=-=-=-=>Episode: 13 | Loss: [1.6239738] | LR: (0.99, 14) | Mean R: 4.715878181818182 | Reward: [-3.135982414483964]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 14<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9837%
nonzero parameters after pruning: 27653 / 30720

[at weight fc2.weight]
percentage of pruned: 10.1488%
nonzero parameters after pruning: 9057 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40252 / 44426 (9.3954%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 49.9870%
nonzero parameters after pruning: 15364 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9603%
nonzero parameters after pruning: 5044 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23618 / 44426 (46.8374%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0521%
nonzero parameters after pruning: 6128 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11130 / 44426 (74.9471%)
-----------------------------------------------------------------------------------------
latency_list: [23.171449714285714, 17.9958957, 8.939926125]
runs_number: [3789109.0, 5542701.0, 10730917.0]
====================Results=======================
--------->Episode: 14,sparsity_level:[0, 2, 4]
--------->Accuracy reward: -1,time reward:8.073966818181818
--------->Reward: 7.073966818181818
==========================================================================================
=-=-=-=-=-=>Episode: 14 | Loss: [1.6219705] | LR: (0.99, 15) | Mean R: 7.073966818181818 | Reward: [-0.7451835316278705]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 15<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9609%
nonzero parameters after pruning: 27660 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9702%
nonzero parameters after pruning: 9075 / 10080

[at weight fc3.weight]
percentage of pruned: 9.8810%
nonzero parameters after pruning: 757 / 840

total pruning rate: 40278 / 44426 (9.3369%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 49.9870%
nonzero parameters after pruning: 15364 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23620 / 44426 (46.8329%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0391%
nonzero parameters after pruning: 6132 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11134 / 44426 (74.9381%)
-----------------------------------------------------------------------------------------
latency_list: [23.187576214285713, 17.9976324, 8.944267875000001]
runs_number: [3786474.0, 5542166.0, 10725708.0]
====================Results=======================
--------->Episode: 15,sparsity_level:[0, 2, 4]
--------->Accuracy reward: -1,time reward:8.070158181818181
--------->Reward: 7.070158181818181
==========================================================================================
=-=-=-=-=-=>Episode: 15 | Loss: [1.5978744] | LR: (0.99, 16) | Mean R: 7.070158181818181 | Reward: [-0.7181727201816042]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 16<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9740%
nonzero parameters after pruning: 27656 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9107%
nonzero parameters after pruning: 9081 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40279 / 44426 (9.3346%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 30.0716%
nonzero parameters after pruning: 21482 / 30720

[at weight fc2.weight]
percentage of pruned: 29.8214%
nonzero parameters after pruning: 7074 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31931 / 44426 (28.1254%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9805%
nonzero parameters after pruning: 6150 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11150 / 44426 (74.9021%)
-----------------------------------------------------------------------------------------
latency_list: [23.188196464285713, 25.21448925, 8.961634875]
runs_number: [3786373.0, 3955895.0, 10704922.0]
====================Results=======================
--------->Episode: 16,sparsity_level:[0, 1, 4]
--------->Accuracy reward: -1,time reward:7.339631818181818
--------->Reward: 6.339631818181818
==========================================================================================
=-=-=-=-=-=>Episode: 16 | Loss: [1.581612] | LR: (0.99, 17) | Mean R: 6.339631818181818 | Reward: [-1.3949849623832211]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 17<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0651%
nonzero parameters after pruning: 15340 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.8810%
nonzero parameters after pruning: 421 / 840

total pruning rate: 23604 / 44426 (46.8690%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9284%
nonzero parameters after pruning: 9238 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7222%
nonzero parameters after pruning: 3052 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15330 / 44426 (65.4932%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0456%
nonzero parameters after pruning: 6130 / 30720

[at weight fc2.weight]
percentage of pruned: 79.7024%
nonzero parameters after pruning: 2046 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11129 / 44426 (74.9494%)
-----------------------------------------------------------------------------------------
latency_list: [12.845527714285714, 10.799010899999999, 8.9388406875]
runs_number: [6834998.0, 9236575.0, 10732220.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7079
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4503
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2247
| test before training | weighted_accuracy   0.5340
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.07 ms/batch  | loss  0.17
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9773
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9745
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9661
| end of epoch   1 | time: 21.89s | weighted_accuracy   0.9742
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9773
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9745
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9661
| end of training | weighted_accuracy   0.9742
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 17,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8246670828925239,time reward:11.138087727272726
--------->Reward: 11.96275481016525
==========================================================================================
=-=-=-=-=-=>Episode: 17 | Loss: [1.6159264] | LR: (0.99, 18) | Mean R: 11.96275481016525 | Reward: [4.097701830532076]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 18<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 29.9870%
nonzero parameters after pruning: 21508 / 30720

[at weight fc2.weight]
percentage of pruned: 29.7817%
nonzero parameters after pruning: 7078 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31961 / 44426 (28.0579%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0000%
nonzero parameters after pruning: 15360 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23627 / 44426 (46.8172%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1042%
nonzero parameters after pruning: 9184 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7024%
nonzero parameters after pruning: 3054 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15277 / 44426 (65.6125%)
-----------------------------------------------------------------------------------------
latency_list: [18.02895696428571, 18.00371085, 13.4412354375]
runs_number: [4869897.0, 5540295.0, 7137261.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9570
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6651
| test_data evaluate | sub loss  0.01 | sub accuracy   0.3339
| test before training | weighted_accuracy   0.7448
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 35.38 ms/batch  | loss  0.16
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9798
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9771
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9706
| end of epoch   1 | time: 41.95s | weighted_accuracy   0.9772
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9798
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9771
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9706
| end of training | weighted_accuracy   0.9772
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 18,sparsity_level:[1, 2, 3]
--------->Accuracy reward: 0.8572224775950115,time reward:6.930660454545454
--------->Reward: 7.787882932140466
==========================================================================================
=-=-=-=-=-=>Episode: 18 | Loss: [1.5811067] | LR: (0.99, 19) | Mean R: 7.787882932140466 | Reward: [-0.07419850300829367]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 19<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9609%
nonzero parameters after pruning: 27660 / 30720

[at weight fc2.weight]
percentage of pruned: 9.8810%
nonzero parameters after pruning: 9084 / 10080

[at weight fc3.weight]
percentage of pruned: 10.1190%
nonzero parameters after pruning: 755 / 840

total pruning rate: 40285 / 44426 (9.3211%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 30.0651%
nonzero parameters after pruning: 21484 / 30720

[at weight fc2.weight]
percentage of pruned: 29.7817%
nonzero parameters after pruning: 7078 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31937 / 44426 (28.1119%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9805%
nonzero parameters after pruning: 6150 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11152 / 44426 (74.8976%)
-----------------------------------------------------------------------------------------
latency_list: [23.191917964285715, 25.21969935, 8.96380575]
runs_number: [3785765.0, 3955078.0, 10702330.0]
====================Results=======================
--------->Episode: 19,sparsity_level:[0, 1, 4]
--------->Accuracy reward: -1,time reward:7.337805909090909
--------->Reward: 6.337805909090909
==========================================================================================
=-=-=-=-=-=>Episode: 19 | Loss: [1.6318175] | LR: (0.99, 20) | Mean R: 6.337805909090909 | Reward: [-1.4794617244681127]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 20<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9707%
nonzero parameters after pruning: 27657 / 30720

[at weight fc2.weight]
percentage of pruned: 10.1190%
nonzero parameters after pruning: 9060 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40259 / 44426 (9.3796%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.8535%
nonzero parameters after pruning: 9261 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7024%
nonzero parameters after pruning: 3054 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15354 / 44426 (65.4392%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0456%
nonzero parameters after pruning: 6130 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11132 / 44426 (74.9426%)
-----------------------------------------------------------------------------------------
latency_list: [23.175791464285712, 10.819851300000002, 8.942097]
runs_number: [3788399.0, 9218784.0, 10728312.0]
====================Results=======================
--------->Episode: 20,sparsity_level:[0, 3, 4]
--------->Accuracy reward: -1,time reward:9.743406818181818
--------->Reward: 8.743406818181818
==========================================================================================
=-=-=-=-=-=>Episode: 20 | Loss: [1.6209244] | LR: (0.99, 21) | Mean R: 8.743406818181818 | Reward: [0.9024587987574284]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 21<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 49.9935%
nonzero parameters after pruning: 15362 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23618 / 44426 (46.8374%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.0944%
nonzero parameters after pruning: 9187 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7024%
nonzero parameters after pruning: 3054 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15281 / 44426 (65.6035%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0521%
nonzero parameters after pruning: 6128 / 30720

[at weight fc2.weight]
percentage of pruned: 79.7024%
nonzero parameters after pruning: 2046 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11128 / 44426 (74.9516%)
-----------------------------------------------------------------------------------------
latency_list: [12.854211214285714, 10.75646175, 8.937755249999999]
runs_number: [6830381.0, 9273112.0, 10733523.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7134
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4100
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2694
| test before training | weighted_accuracy   0.5336
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 25.19 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9799
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9733
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9651
| end of epoch   1 | time: 27.76s | weighted_accuracy   0.9750
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9799
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9733
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9651
| end of training | weighted_accuracy   0.9750
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 21,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8328885502285428,time reward:11.153189090909091
--------->Reward: 11.986077641137634
==========================================================================================
=-=-=-=-=-=>Episode: 21 | Loss: [1.6345532] | LR: (0.99, 22) | Mean R: 11.986077641137634 | Reward: [4.03689718996482]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 22<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9837%
nonzero parameters after pruning: 27653 / 30720

[at weight fc2.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 9072 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40267 / 44426 (9.3616%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8810%
nonzero parameters after pruning: 5052 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23620 / 44426 (46.8329%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9089%
nonzero parameters after pruning: 9244 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7024%
nonzero parameters after pruning: 3054 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15338 / 44426 (65.4752%)
-----------------------------------------------------------------------------------------
latency_list: [23.180753464285715, 17.9976324, 13.507447124999999]
runs_number: [3787589.0, 5542166.0, 7102275.0]
====================Results=======================
--------->Episode: 22,sparsity_level:[0, 2, 3]
--------->Accuracy reward: -1,time reward:6.423649999999999
--------->Reward: 5.423649999999999
==========================================================================================
=-=-=-=-=-=>Episode: 22 | Loss: [1.599329] | LR: (0.99, 23) | Mean R: 5.423649999999999 | Reward: [-2.463845100006078]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 23<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0586%
nonzero parameters after pruning: 15342 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8810%
nonzero parameters after pruning: 5052 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23604 / 44426 (46.8690%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.8568%
nonzero parameters after pruning: 9260 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 3048 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15348 / 44426 (65.4527%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0456%
nonzero parameters after pruning: 6130 / 30720

[at weight fc2.weight]
percentage of pruned: 80.3175%
nonzero parameters after pruning: 1984 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11067 / 44426 (75.0889%)
-----------------------------------------------------------------------------------------
latency_list: [12.845527714285714, 10.8146412, 8.871543562500001]
runs_number: [6834998.0, 9223226.0, 10813632.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7417
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2530
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2896
| test before training | weighted_accuracy   0.5047
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 37.50 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9777
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9745
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9599
| end of epoch   1 | time: 42.74s | weighted_accuracy   0.9732
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9777
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9745
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9599
| end of training | weighted_accuracy   0.9732
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 23,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8131117290920682,time reward:11.169025454545453
--------->Reward: 11.982137183637521
==========================================================================================
=-=-=-=-=-=>Episode: 23 | Loss: [1.6118222] | LR: (0.99, 24) | Mean R: 11.982137183637521 | Reward: [3.99551397421609]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 24<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 49.9870%
nonzero parameters after pruning: 15364 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8611%
nonzero parameters after pruning: 5054 / 10080

[at weight fc3.weight]
percentage of pruned: 49.7619%
nonzero parameters after pruning: 422 / 840

total pruning rate: 23626 / 44426 (46.8194%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7222%
nonzero parameters after pruning: 3052 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15328 / 44426 (65.4977%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.1172%
nonzero parameters after pruning: 6108 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11110 / 44426 (74.9921%)
-----------------------------------------------------------------------------------------
latency_list: [12.859173214285715, 10.7972742, 8.918217375000001]
runs_number: [6827745.0, 9238061.0, 10757038.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7234
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2489
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1055
| test before training | weighted_accuracy   0.4575
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 38.25 ms/batch  | loss  0.22
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9741
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9735
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9600
| end of epoch   1 | time: 42.91s | weighted_accuracy   0.9711
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9741
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9735
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9600
| end of training | weighted_accuracy   0.9711
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 24,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.789999696943495,time reward:11.146747272727271
--------->Reward: 11.936746969670766
==========================================================================================
=-=-=-=-=-=>Episode: 24 | Loss: [1.5545529] | LR: (0.99, 25) | Mean R: 11.936746969670766 | Reward: [3.855918156106666]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 25<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9707%
nonzero parameters after pruning: 27657 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9008%
nonzero parameters after pruning: 9082 / 10080

[at weight fc3.weight]
percentage of pruned: 10.1190%
nonzero parameters after pruning: 755 / 840

total pruning rate: 40280 / 44426 (9.3324%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 29.9740%
nonzero parameters after pruning: 21512 / 30720

[at weight fc2.weight]
percentage of pruned: 29.8115%
nonzero parameters after pruning: 7075 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31962 / 44426 (28.0556%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.0781%
nonzero parameters after pruning: 9192 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7222%
nonzero parameters after pruning: 3052 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15284 / 44426 (65.5967%)
-----------------------------------------------------------------------------------------
latency_list: [23.188816714285714, 25.2414081, 13.448833500000001]
runs_number: [3786272.0, 3951676.0, 7133228.0]
====================Results=======================
--------->Episode: 25,sparsity_level:[0, 1, 3]
--------->Accuracy reward: -1,time reward:5.7141709090909085
--------->Reward: 4.7141709090909085
==========================================================================================
=-=-=-=-=-=>Episode: 25 | Loss: [1.5905892] | LR: (0.99, 26) | Mean R: 4.7141709090909085 | Reward: [-3.296016306104706]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 26<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 30.1172%
nonzero parameters after pruning: 21468 / 30720

[at weight fc2.weight]
percentage of pruned: 29.8214%
nonzero parameters after pruning: 7074 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31917 / 44426 (28.1569%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9603%
nonzero parameters after pruning: 5044 / 10080

[at weight fc3.weight]
percentage of pruned: 49.7619%
nonzero parameters after pruning: 422 / 840

total pruning rate: 23610 / 44426 (46.8554%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.8535%
nonzero parameters after pruning: 9261 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15356 / 44426 (65.4347%)
-----------------------------------------------------------------------------------------
latency_list: [18.001665964285714, 17.9889489, 13.526985000000002]
runs_number: [4877280.0, 5544842.0, 7092017.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9306
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7168
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4053
| test before training | weighted_accuracy   0.7614
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 39.26 ms/batch  | loss  0.16
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9823
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9744
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9694
| end of epoch   1 | time: 44.92s | weighted_accuracy   0.9774
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9823
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9744
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9694
| end of training | weighted_accuracy   0.9774
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 26,sparsity_level:[1, 2, 3]
--------->Accuracy reward: 0.8594450685713027,time reward:6.915517727272727
--------->Reward: 7.774962795844029
==========================================================================================
=-=-=-=-=-=>Episode: 26 | Loss: [1.5575112] | LR: (0.99, 27) | Mean R: 7.774962795844029 | Reward: [-0.23258891029860074]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 27<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 10.0846%
nonzero parameters after pruning: 27622 / 30720

[at weight fc2.weight]
percentage of pruned: 10.1488%
nonzero parameters after pruning: 9057 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40221 / 44426 (9.4652%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 29.9870%
nonzero parameters after pruning: 21508 / 30720

[at weight fc2.weight]
percentage of pruned: 29.8214%
nonzero parameters after pruning: 7074 / 10080

[at weight fc3.weight]
percentage of pruned: 30.0000%
nonzero parameters after pruning: 588 / 840

total pruning rate: 31956 / 44426 (28.0691%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1042%
nonzero parameters after pruning: 9184 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6429%
nonzero parameters after pruning: 3060 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15283 / 44426 (65.5990%)
-----------------------------------------------------------------------------------------
latency_list: [23.152221964285715, 25.236198, 13.4477480625]
runs_number: [3792256.0, 3952492.0, 7133804.0]
====================Results=======================
--------->Episode: 27,sparsity_level:[0, 1, 3]
--------->Accuracy reward: -1,time reward:5.717523636363636
--------->Reward: 4.717523636363636
==========================================================================================
=-=-=-=-=-=>Episode: 27 | Loss: [1.6386583] | LR: (0.99, 28) | Mean R: 4.717523636363636 | Reward: [-3.2239590780328227]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 28<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 10.0749%
nonzero parameters after pruning: 27625 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9802%
nonzero parameters after pruning: 9074 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40241 / 44426 (9.4202%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 30.0130%
nonzero parameters after pruning: 21500 / 30720

[at weight fc2.weight]
percentage of pruned: 29.7917%
nonzero parameters after pruning: 7077 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31952 / 44426 (28.0782%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1139%
nonzero parameters after pruning: 9181 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15276 / 44426 (65.6147%)
-----------------------------------------------------------------------------------------
latency_list: [23.164626964285716, 25.2327246, 13.44015]
runs_number: [3790225.0, 3953036.0, 7137837.0]
====================Results=======================
--------->Episode: 28,sparsity_level:[0, 1, 3]
--------->Accuracy reward: -1,time reward:5.718680909090909
--------->Reward: 4.718680909090909
==========================================================================================
=-=-=-=-=-=>Episode: 28 | Loss: [1.5532211] | LR: (0.99, 29) | Mean R: 4.718680909090909 | Reward: [-3.158833575989741]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 29<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0195%
nonzero parameters after pruning: 15354 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9603%
nonzero parameters after pruning: 5044 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23608 / 44426 (46.8599%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7024%
nonzero parameters after pruning: 3054 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15330 / 44426 (65.4932%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9935%
nonzero parameters after pruning: 6146 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11146 / 44426 (74.9111%)
-----------------------------------------------------------------------------------------
latency_list: [12.848008714285713, 10.799010899999999, 8.957293125]
runs_number: [6833679.0, 9236575.0, 10710111.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7033
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2892
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1346
| test before training | weighted_accuracy   0.4653
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 38.84 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9741
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9754
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9660
| end of epoch   1 | time: 44.64s | weighted_accuracy   0.9729
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9741
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9754
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9660
| end of training | weighted_accuracy   0.9729
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 29,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8096665806240506,time reward:11.127438636363635
--------->Reward: 11.437105216987685
==========================================================================================
=-=-=-=-=-=>Episode: 29 | Loss: [1.6005398] | LR: (0.99, 30) | Mean R: 11.437105216987685 | Reward: [3.491557182070941]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 30<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9740%
nonzero parameters after pruning: 27656 / 30720

[at weight fc2.weight]
percentage of pruned: 10.1488%
nonzero parameters after pruning: 9057 / 10080

[at weight fc3.weight]
percentage of pruned: 10.1190%
nonzero parameters after pruning: 755 / 840

total pruning rate: 40254 / 44426 (9.3909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 30.0000%
nonzero parameters after pruning: 21504 / 30720

[at weight fc2.weight]
percentage of pruned: 29.7619%
nonzero parameters after pruning: 7080 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31959 / 44426 (28.0624%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.8568%
nonzero parameters after pruning: 9260 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7024%
nonzero parameters after pruning: 3054 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15354 / 44426 (65.4392%)
-----------------------------------------------------------------------------------------
latency_list: [23.172690214285716, 25.23880305, 13.524814124999999]
runs_number: [3788906.0, 3952084.0, 7093155.0]
====================Results=======================
--------->Episode: 30,sparsity_level:[0, 1, 3]
--------->Accuracy reward: -1,time reward:5.697338636363636
--------->Reward: 4.697338636363636
==========================================================================================
=-=-=-=-=-=>Episode: 30 | Loss: [1.589173] | LR: (0.99, 31) | Mean R: 4.697338636363636 | Reward: [-3.187950176600162]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 31<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 10.0651%
nonzero parameters after pruning: 27628 / 30720

[at weight fc2.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 9072 / 10080

[at weight fc3.weight]
percentage of pruned: 10.1190%
nonzero parameters after pruning: 755 / 840

total pruning rate: 40241 / 44426 (9.4202%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.7619%
nonzero parameters after pruning: 422 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.0879%
nonzero parameters after pruning: 9189 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15283 / 44426 (65.5990%)
-----------------------------------------------------------------------------------------
latency_list: [23.164626964285716, 17.9906856, 13.4477480625]
runs_number: [3790225.0, 5544306.0, 7133804.0]
====================Results=======================
--------->Episode: 31,sparsity_level:[0, 2, 3]
--------->Accuracy reward: -1,time reward:6.440152272727272
--------->Reward: 5.440152272727272
==========================================================================================
=-=-=-=-=-=>Episode: 31 | Loss: [1.6059729] | LR: (0.99, 32) | Mean R: 5.440152272727272 | Reward: [-2.4001519794281654]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 32<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9740%
nonzero parameters after pruning: 27656 / 30720

[at weight fc2.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 9072 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40270 / 44426 (9.3549%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0130%
nonzero parameters after pruning: 15356 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9008%
nonzero parameters after pruning: 5050 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23616 / 44426 (46.8419%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1074%
nonzero parameters after pruning: 9183 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6429%
nonzero parameters after pruning: 3060 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15282 / 44426 (65.6012%)
-----------------------------------------------------------------------------------------
latency_list: [23.182614214285714, 17.994159000000003, 13.446662625]
runs_number: [3787285.0, 5543236.0, 7134380.0]
====================Results=======================
--------->Episode: 32,sparsity_level:[0, 2, 3]
--------->Accuracy reward: -1,time reward:6.438591363636363
--------->Reward: 5.438591363636363
==========================================================================================
=-=-=-=-=-=>Episode: 32 | Loss: [1.5872083] | LR: (0.99, 33) | Mean R: 5.438591363636363 | Reward: [-2.3580610347195323]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 33<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 30.0130%
nonzero parameters after pruning: 21500 / 30720

[at weight fc2.weight]
percentage of pruned: 29.8512%
nonzero parameters after pruning: 7071 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31946 / 44426 (28.0917%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0130%
nonzero parameters after pruning: 15356 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9008%
nonzero parameters after pruning: 5050 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23616 / 44426 (46.8419%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9609%
nonzero parameters after pruning: 6156 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6429%
nonzero parameters after pruning: 2052 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11161 / 44426 (74.8773%)
-----------------------------------------------------------------------------------------
latency_list: [18.019653214285714, 17.994159000000003, 8.973574687500001]
runs_number: [4872411.0, 5543236.0, 10690679.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8959
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6441
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2507
| test before training | weighted_accuracy   0.6913
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 38.68 ms/batch  | loss  0.22
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9798
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9749
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9595
| end of epoch   1 | time: 44.37s | weighted_accuracy   0.9743
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9798
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9749
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9595
| end of training | weighted_accuracy   0.9743
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 33,sparsity_level:[1, 2, 4]
--------->Accuracy reward: 0.8252220683627659,time reward:8.54833
--------->Reward: 9.373552068362766
==========================================================================================
=-=-=-=-=-=>Episode: 33 | Loss: [1.5819502] | LR: (0.99, 34) | Mean R: 9.373552068362766 | Reward: [1.5520387214732896]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 34<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9740%
nonzero parameters after pruning: 27656 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9405%
nonzero parameters after pruning: 9078 / 10080

[at weight fc3.weight]
percentage of pruned: 10.1190%
nonzero parameters after pruning: 755 / 840

total pruning rate: 40275 / 44426 (9.3436%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9284%
nonzero parameters after pruning: 9238 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6429%
nonzero parameters after pruning: 3060 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15337 / 44426 (65.4774%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9479%
nonzero parameters after pruning: 6160 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11160 / 44426 (74.8796%)
-----------------------------------------------------------------------------------------
latency_list: [23.185715464285714, 10.80508935, 8.972489250000002]
runs_number: [3786778.0, 9231379.0, 10691972.0]
====================Results=======================
--------->Episode: 34,sparsity_level:[0, 3, 4]
--------->Accuracy reward: -1,time reward:9.731876818181817
--------->Reward: 8.731876818181817
==========================================================================================
=-=-=-=-=-=>Episode: 34 | Loss: [1.5346361] | LR: (0.99, 35) | Mean R: 8.731876818181817 | Reward: [0.8967409817740588]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 35<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0130%
nonzero parameters after pruning: 15356 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.7619%
nonzero parameters after pruning: 422 / 840

total pruning rate: 23622 / 44426 (46.8284%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1042%
nonzero parameters after pruning: 9184 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7024%
nonzero parameters after pruning: 3054 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15278 / 44426 (65.6102%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11156 / 44426 (74.8886%)
-----------------------------------------------------------------------------------------
latency_list: [12.856692214285713, 10.7538567, 8.9681475]
runs_number: [6829063.0, 9275358.0, 10697148.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7382
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2912
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2228
| test before training | weighted_accuracy   0.5010
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 38.41 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9756
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9711
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9634
| end of epoch   1 | time: 44.22s | weighted_accuracy   0.9718
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9756
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9711
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9634
| end of training | weighted_accuracy   0.9718
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 35,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7978887028164334,time reward:11.137076818181818
--------->Reward: 11.93496552099825
==========================================================================================
=-=-=-=-=-=>Episode: 35 | Loss: [1.5683742] | LR: (0.99, 36) | Mean R: 11.93496552099825 | Reward: [4.034846172460098]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 36<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 10.0618%
nonzero parameters after pruning: 27629 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9206%
nonzero parameters after pruning: 9080 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40251 / 44426 (9.3977%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9316%
nonzero parameters after pruning: 9237 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15332 / 44426 (65.4887%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [23.17082946428571, 10.8007476, 8.965976625]
runs_number: [3789211.0, 9235090.0, 10699738.0]
====================Results=======================
--------->Episode: 36,sparsity_level:[0, 3, 4]
--------->Accuracy reward: -1,time reward:9.738199545454545
--------->Reward: 8.738199545454545
==========================================================================================
=-=-=-=-=-=>Episode: 36 | Loss: [1.4570234] | LR: (0.99, 37) | Mean R: 8.738199545454545 | Reward: [0.8249329593167118]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 37<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 9.9707%
nonzero parameters after pruning: 27657 / 30720

[at weight fc2.weight]
percentage of pruned: 10.1488%
nonzero parameters after pruning: 9057 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40256 / 44426 (9.3864%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 49.9935%
nonzero parameters after pruning: 15362 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9008%
nonzero parameters after pruning: 5050 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23622 / 44426 (46.8284%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.8214%
nonzero parameters after pruning: 2034 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11142 / 44426 (74.9201%)
-----------------------------------------------------------------------------------------
latency_list: [23.17393071428571, 17.999369100000003, 8.952951375]
runs_number: [3788704.0, 5541632.0, 10715305.0]
====================Results=======================
--------->Episode: 37,sparsity_level:[0, 2, 4]
--------->Accuracy reward: -1,time reward:8.066200454545454
--------->Reward: 7.066200454545454
==========================================================================================
=-=-=-=-=-=>Episode: 37 | Loss: [1.5148234] | LR: (0.99, 38) | Mean R: 7.066200454545454 | Reward: [-0.8343243478919282]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 38<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 49.9935%
nonzero parameters after pruning: 15362 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23630 / 44426 (46.8104%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.8568%
nonzero parameters after pruning: 9260 / 30720

[at weight fc2.weight]
percentage of pruned: 69.9008%
nonzero parameters after pruning: 3034 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15334 / 44426 (65.4842%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0521%
nonzero parameters after pruning: 6128 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11131 / 44426 (74.9449%)
-----------------------------------------------------------------------------------------
latency_list: [12.861654214285714, 10.8024843, 8.9410115625]
runs_number: [6826428.0, 9233605.0, 10729614.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6977
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2769
| test before training | weighted_accuracy   0.4586
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 38.48 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9780
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9689
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9656
| end of epoch   1 | time: 44.26s | weighted_accuracy   0.9728
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9780
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9689
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9656
| end of training | weighted_accuracy   0.9728
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 38,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8087778091430664,time reward:11.131657727272726
--------->Reward: 11.940435536415793
==========================================================================================
=-=-=-=-=-=>Episode: 38 | Loss: [1.5269431] | LR: (0.99, 39) | Mean R: 11.940435536415793 | Reward: [3.979761159665169]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 39<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 30.0846%
nonzero parameters after pruning: 21478 / 30720

[at weight fc2.weight]
percentage of pruned: 29.8214%
nonzero parameters after pruning: 7074 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31927 / 44426 (28.1344%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1042%
nonzero parameters after pruning: 9184 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7222%
nonzero parameters after pruning: 3052 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15276 / 44426 (65.6147%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9544%
nonzero parameters after pruning: 6158 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6429%
nonzero parameters after pruning: 2052 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11163 / 44426 (74.8728%)
-----------------------------------------------------------------------------------------
latency_list: [18.007868464285714, 10.752120000000001, 8.9757455625]
runs_number: [4875600.0, 9276857.0, 10688093.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8254
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3574
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2032
| test before training | weighted_accuracy   0.5606
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 39.14 ms/batch  | loss  0.21
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9803
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9726
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9641
| end of epoch   1 | time: 45.15s | weighted_accuracy   0.9747
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9803
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9726
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9641
| end of training | weighted_accuracy   0.9747
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 39,sparsity_level:[1, 3, 4]
--------->Accuracy reward: 0.8305553595225017,time reward:10.245704545454545
--------->Reward: 11.076259904977046
==========================================================================================
=-=-=-=-=-=>Episode: 39 | Loss: [1.5241374] | LR: (0.99, 40) | Mean R: 11.076259904977046 | Reward: [3.0694597221130238]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 40<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 10.0651%
nonzero parameters after pruning: 27628 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9702%
nonzero parameters after pruning: 9075 / 10080

[at weight fc3.weight]
percentage of pruned: 9.8810%
nonzero parameters after pruning: 757 / 840

total pruning rate: 40246 / 44426 (9.4089%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 50.0391%
nonzero parameters after pruning: 15348 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9603%
nonzero parameters after pruning: 5044 / 10080

[at weight fc3.weight]
percentage of pruned: 49.7619%
nonzero parameters after pruning: 422 / 840

total pruning rate: 23600 / 44426 (46.8780%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9284%
nonzero parameters after pruning: 9238 / 30720

[at weight fc2.weight]
percentage of pruned: 69.9603%
nonzero parameters after pruning: 3028 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15305 / 44426 (65.5495%)
-----------------------------------------------------------------------------------------
latency_list: [23.167728214285713, 17.9802654, 13.4716276875]
runs_number: [3789718.0, 5547519.0, 7121159.0]
====================Results=======================
--------->Episode: 40,sparsity_level:[0, 2, 3]
--------->Accuracy reward: -1,time reward:6.435634545454545
--------->Reward: 5.435634545454545
==========================================================================================
=-=-=-=-=-=>Episode: 40 | Loss: [1.4923971] | LR: (0.99, 41) | Mean R: 5.435634545454545 | Reward: [-2.536244684867931]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 41<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 10.0781%
nonzero parameters after pruning: 27624 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9008%
nonzero parameters after pruning: 9082 / 10080

[at weight fc3.weight]
percentage of pruned: 10.0000%
nonzero parameters after pruning: 756 / 840

total pruning rate: 40248 / 44426 (9.4044%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 29.9740%
nonzero parameters after pruning: 21512 / 30720

[at weight fc2.weight]
percentage of pruned: 29.7222%
nonzero parameters after pruning: 7084 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31971 / 44426 (28.0354%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 49.9870%
nonzero parameters after pruning: 15364 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.7619%
nonzero parameters after pruning: 422 / 840

total pruning rate: 23629 / 44426 (46.8127%)
-----------------------------------------------------------------------------------------
latency_list: [23.16896871428571, 25.24922325, 22.506809437500003]
runs_number: [3789515.0, 3950453.0, 4262425.0]
====================Results=======================
--------->Episode: 41,sparsity_level:[0, 1, 2]
--------->Accuracy reward: -1,time reward:4.410178636363636
--------->Reward: 3.4101786363636357
==========================================================================================
=-=-=-=-=-=>Episode: 41 | Loss: [1.4007431] | LR: (0.99, 42) | Mean R: 3.4101786363636357 | Reward: [-4.499523528854228]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 42<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23626 / 44426 (46.8194%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9089%
nonzero parameters after pruning: 9244 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15338 / 44426 (65.4752%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0521%
nonzero parameters after pruning: 6128 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11128 / 44426 (74.9516%)
-----------------------------------------------------------------------------------------
latency_list: [12.859173214285715, 10.8059577, 8.937755249999999]
runs_number: [6827745.0, 9230637.0, 10733523.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7355
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3328
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2610
| test before training | weighted_accuracy   0.5198
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 38.97 ms/batch  | loss  0.17
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9781
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9721
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9626
| end of epoch   1 | time: 44.74s | weighted_accuracy   0.9732
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9781
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9721
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9626
| end of training | weighted_accuracy   0.9732
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 42,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8133335908253988,time reward:11.132684090909091
--------->Reward: 11.94601768173449
==========================================================================================
=-=-=-=-=-=>Episode: 42 | Loss: [1.5448917] | LR: (0.99, 43) | Mean R: 11.94601768173449 | Reward: [3.9818066787620783]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 43<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23626 / 44426 (46.8194%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.0977%
nonzero parameters after pruning: 9186 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7024%
nonzero parameters after pruning: 3054 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15280 / 44426 (65.6057%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0716%
nonzero parameters after pruning: 6122 / 30720

[at weight fc2.weight]
percentage of pruned: 79.7024%
nonzero parameters after pruning: 2046 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11121 / 44426 (74.9674%)
-----------------------------------------------------------------------------------------
latency_list: [12.859173214285715, 10.7555934, 8.9301571875]
runs_number: [6827745.0, 9273861.0, 10742656.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6122
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3807
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1472
| test before training | weighted_accuracy   0.4498
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 38.33 ms/batch  | loss  0.21
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9662
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9683
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9649
| end of epoch   1 | time: 44.15s | weighted_accuracy   0.9666
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9662
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9683
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9649
| end of training | weighted_accuracy   0.9666
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 43,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.739666223526001,time reward:11.156482727272726
--------->Reward: 11.396148950798727
==========================================================================================
=-=-=-=-=-=>Episode: 43 | Loss: [1.4358405] | LR: (0.99, 44) | Mean R: 11.396148950798727 | Reward: [3.3859086050737197]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 44<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 49.9935%
nonzero parameters after pruning: 15362 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8115%
nonzero parameters after pruning: 5059 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23631 / 44426 (46.8082%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9089%
nonzero parameters after pruning: 9244 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6429%
nonzero parameters after pruning: 3060 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15344 / 44426 (65.4617%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9609%
nonzero parameters after pruning: 6156 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11156 / 44426 (74.8886%)
-----------------------------------------------------------------------------------------
latency_list: [12.862274464285713, 10.8111678, 8.9681475]
runs_number: [6826099.0, 9226189.0, 10697148.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7289
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4446
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2824
| test before training | weighted_accuracy   0.5543
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 38.41 ms/batch  | loss  0.20
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9746
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9726
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9648
| end of epoch   1 | time: 44.14s | weighted_accuracy   0.9720
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9746
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9726
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9648
| end of training | weighted_accuracy   0.9720
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 44,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.800444417529636,time reward:11.11338
--------->Reward: 11.913824417529636
==========================================================================================
=-=-=-=-=-=>Episode: 44 | Loss: [1.3939555] | LR: (0.99, 45) | Mean R: 11.913824417529636 | Reward: [3.8519093612240756]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 45<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 10.0749%
nonzero parameters after pruning: 27625 / 30720

[at weight fc2.weight]
percentage of pruned: 9.9107%
nonzero parameters after pruning: 9081 / 10080

[at weight fc3.weight]
percentage of pruned: 10.1190%
nonzero parameters after pruning: 755 / 840

total pruning rate: 40247 / 44426 (9.4067%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 30.0000%
nonzero parameters after pruning: 21504 / 30720

[at weight fc2.weight]
percentage of pruned: 29.7718%
nonzero parameters after pruning: 7079 / 10080

[at weight fc3.weight]
percentage of pruned: 29.8810%
nonzero parameters after pruning: 589 / 840

total pruning rate: 31958 / 44426 (28.0646%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [23.168348464285714, 25.2379347, 8.965976625]
runs_number: [3789617.0, 3952220.0, 10699738.0]
====================Results=======================
--------->Episode: 45,sparsity_level:[0, 1, 4]
--------->Accuracy reward: -1,time reward:7.337079545454545
--------->Reward: 6.337079545454545
==========================================================================================
=-=-=-=-=-=>Episode: 45 | Loss: [1.2747099] | LR: (0.99, 46) | Mean R: 6.337079545454545 | Reward: [-1.7049255169688209]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 46<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0130%
nonzero parameters after pruning: 15356 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8512%
nonzero parameters after pruning: 5055 / 10080

[at weight fc3.weight]
percentage of pruned: 49.7619%
nonzero parameters after pruning: 422 / 840

total pruning rate: 23619 / 44426 (46.8352%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9512%
nonzero parameters after pruning: 6159 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11161 / 44426 (74.8773%)
-----------------------------------------------------------------------------------------
latency_list: [12.854831464285713, 10.75125165, 8.973574687500001]
runs_number: [6830052.0, 9277606.0, 10690679.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7879
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4505
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1048
| test before training | weighted_accuracy   0.5501
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 39.13 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9777
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9751
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9649
| end of epoch   1 | time: 44.96s | weighted_accuracy   0.9744
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9777
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9751
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9649
| end of training | weighted_accuracy   0.9744
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 46,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8262221018473308,time reward:11.135607727272726
--------->Reward: 11.961829829120056
==========================================================================================
=-=-=-=-=-=>Episode: 46 | Loss: [1.3900825] | LR: (0.99, 47) | Mean R: 11.961829829120056 | Reward: [3.869866014804529]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 47<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0130%
nonzero parameters after pruning: 15356 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.0911%
nonzero parameters after pruning: 9188 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15283 / 44426 (65.5990%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9935%
nonzero parameters after pruning: 6146 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6429%
nonzero parameters after pruning: 2052 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11151 / 44426 (74.8998%)
-----------------------------------------------------------------------------------------
latency_list: [12.850489714285713, 10.75819845, 8.9627203125]
runs_number: [6832359.0, 9271615.0, 10703626.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6710
| test_data evaluate | sub loss  0.00 | sub accuracy   0.5024
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2088
| test before training | weighted_accuracy   0.5280
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.09 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9749
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9697
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9633
| end of epoch   1 | time: 21.92s | weighted_accuracy   0.9710
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9749
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9697
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9633
| end of training | weighted_accuracy   0.9710
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 47,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7891109254625108,time reward:11.139818181818182
--------->Reward: 11.928929107280693
==========================================================================================
=-=-=-=-=-=>Episode: 47 | Loss: [1.2502438] | LR: (0.99, 48) | Mean R: 11.928929107280693 | Reward: [3.788489518716476]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 48<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0195%
nonzero parameters after pruning: 15354 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8413%
nonzero parameters after pruning: 5056 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23620 / 44426 (46.8329%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.8568%
nonzero parameters after pruning: 9260 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15355 / 44426 (65.4369%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.8214%
nonzero parameters after pruning: 2034 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11139 / 44426 (74.9268%)
-----------------------------------------------------------------------------------------
latency_list: [12.855451714285714, 10.820719650000001, 8.9496950625]
runs_number: [6829722.0, 9218044.0, 10719204.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6396
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4050
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1723
| test before training | weighted_accuracy   0.4758
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.03 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9777
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9715
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9648
| end of epoch   1 | time: 21.85s | weighted_accuracy   0.9733
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9777
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9715
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9648
| end of training | weighted_accuracy   0.9733
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 48,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8140005005730523,time reward:11.12135
--------->Reward: 11.935350500573051
==========================================================================================
=-=-=-=-=-=>Episode: 48 | Loss: [1.151832] | LR: (0.99, 49) | Mean R: 11.935350500573051 | Reward: [3.747411767745511]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 49<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8512%
nonzero parameters after pruning: 5055 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23609 / 44426 (46.8577%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1139%
nonzero parameters after pruning: 9181 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7817%
nonzero parameters after pruning: 3046 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15267 / 44426 (65.6350%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6429%
nonzero parameters after pruning: 2052 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11159 / 44426 (74.8818%)
-----------------------------------------------------------------------------------------
latency_list: [12.848628964285714, 10.744304849999999, 8.9714038125]
runs_number: [6833349.0, 9283604.0, 10693266.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7917
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3646
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3473
| test before training | weighted_accuracy   0.5747
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.11 ms/batch  | loss  0.17
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9775
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9677
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9590
| end of epoch   1 | time: 21.95s | weighted_accuracy   0.9709
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9775
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9677
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9590
| end of training | weighted_accuracy   0.9709
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 49,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7873333825005425,time reward:11.141008636363635
--------->Reward: 11.928342018864177
==========================================================================================
=-=-=-=-=-=>Episode: 49 | Loss: [0.9986239] | LR: (0.495, 50) | Mean R: 11.928342018864177 | Reward: [3.6940156628871215]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 50<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
************Process:********** 16.666666666666664%
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8115%
nonzero parameters after pruning: 5059 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23613 / 44426 (46.8487%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7817%
nonzero parameters after pruning: 3046 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15266 / 44426 (65.6372%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9870%
nonzero parameters after pruning: 6148 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6429%
nonzero parameters after pruning: 2052 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11153 / 44426 (74.8953%)
-----------------------------------------------------------------------------------------
latency_list: [12.851109964285714, 10.7434365, 8.964891187500001]
runs_number: [6832029.0, 9284355.0, 10701034.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8343
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3791
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1469
| test before training | weighted_accuracy   0.5603
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.01 ms/batch  | loss  0.17
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9787
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9740
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9651
| end of epoch   1 | time: 21.99s | weighted_accuracy   0.9746
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9787
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9740
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9651
| end of training | weighted_accuracy   0.9746
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 50,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8285552925533719,time reward:11.144280909090908
--------->Reward: 11.972836201644279
==========================================================================================
=-=-=-=-=-=>Episode: 50 | Loss: [0.9539469] | LR: (0.495, 51) | Mean R: 11.972836201644279 | Reward: [3.6926194431804564]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 51<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0000%
nonzero parameters after pruning: 15360 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8413%
nonzero parameters after pruning: 5056 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23626 / 44426 (46.8194%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9479%
nonzero parameters after pruning: 6160 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6429%
nonzero parameters after pruning: 2052 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11165 / 44426 (74.8683%)
-----------------------------------------------------------------------------------------
latency_list: [12.859173214285715, 10.75125165, 8.977916437500001]
runs_number: [6827745.0, 9277606.0, 10685509.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6695
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4505
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2125
| test before training | weighted_accuracy   0.5124
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.05 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9789
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9701
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9624
| end of epoch   1 | time: 21.92s | weighted_accuracy   0.9730
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9789
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9701
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9624
| end of training | weighted_accuracy   0.9730
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 51,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8106666141086155,time reward:11.13220909090909
--------->Reward: 11.942875705017705
==========================================================================================
=-=-=-=-=-=>Episode: 51 | Loss: [0.8361534] | LR: (0.495, 52) | Mean R: 11.942875705017705 | Reward: [3.618092686908353]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 52<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 49.9870%
nonzero parameters after pruning: 15364 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8115%
nonzero parameters after pruning: 5059 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23633 / 44426 (46.8037%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.8535%
nonzero parameters after pruning: 9261 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7024%
nonzero parameters after pruning: 3054 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15354 / 44426 (65.4392%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9740%
nonzero parameters after pruning: 6152 / 30720

[at weight fc2.weight]
percentage of pruned: 79.7024%
nonzero parameters after pruning: 2046 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11151 / 44426 (74.8998%)
-----------------------------------------------------------------------------------------
latency_list: [12.863514964285715, 10.819851300000002, 8.9627203125]
runs_number: [6825441.0, 9218784.0, 10703626.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7696
| test_data evaluate | sub loss  0.01 | sub accuracy   0.3090
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1333
| test before training | weighted_accuracy   0.5042
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.03 ms/batch  | loss  0.17
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9768
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9719
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9682
| end of epoch   1 | time: 21.87s | weighted_accuracy   0.9736
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9768
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9719
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9682
| end of training | weighted_accuracy   0.9736
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 52,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8178887102339003,time reward:11.112659545454544
--------->Reward: 11.930548255688445
==========================================================================================
=-=-=-=-=-=>Episode: 52 | Loss: [0.98197424] | LR: (0.495, 53) | Mean R: 11.930548255688445 | Reward: [3.5622928117602246]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 53<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9740%
nonzero parameters after pruning: 6152 / 30720

[at weight fc2.weight]
percentage of pruned: 79.7024%
nonzero parameters after pruning: 2046 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11151 / 44426 (74.8998%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.75125165, 8.9627203125]
runs_number: [6832689.0, 9277606.0, 10703626.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8561
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4822
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1739
| test before training | weighted_accuracy   0.6075
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.06 ms/batch  | loss  0.17
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9772
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9751
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9671
| end of epoch   1 | time: 21.98s | weighted_accuracy   0.9745
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9772
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9751
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9671
| end of training | weighted_accuracy   0.9745
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 53,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8283327685462104,time reward:11.142691363636363
--------->Reward: 11.971024132182574
==========================================================================================
=-=-=-=-=-=>Episode: 53 | Loss: [0.8994222] | LR: (0.495, 54) | Mean R: 11.971024132182574 | Reward: [3.5597927663428326]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 54<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8115%
nonzero parameters after pruning: 5059 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23627 / 44426 (46.8172%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9414%
nonzero parameters after pruning: 6162 / 30720

[at weight fc2.weight]
percentage of pruned: 79.8214%
nonzero parameters after pruning: 2034 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11149 / 44426 (74.9043%)
-----------------------------------------------------------------------------------------
latency_list: [12.859793464285714, 10.79987925, 8.9605494375]
runs_number: [6827416.0, 9235832.0, 10706219.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.01 | sub accuracy   0.5191
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4526
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1942
| test before training | weighted_accuracy   0.4342
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.96 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9752
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9614
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9569
| end of epoch   1 | time: 21.81s | weighted_accuracy   0.9674
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9752
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9614
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9569
| end of training | weighted_accuracy   0.9674
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 54,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7488890488942465,time reward:11.122485
--------->Reward: 11.871374048894246
==========================================================================================
=-=-=-=-=-=>Episode: 54 | Loss: [0.68534374] | LR: (0.495, 55) | Mean R: 11.871374048894246 | Reward: [3.4191356238868345]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 55<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8611%
nonzero parameters after pruning: 5054 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23622 / 44426 (46.8284%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9479%
nonzero parameters after pruning: 6160 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11160 / 44426 (74.8796%)
-----------------------------------------------------------------------------------------
latency_list: [12.856692214285713, 10.75125165, 8.972489249999999]
runs_number: [6829063.0, 9277606.0, 10691972.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8746
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4122
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1761
| test before training | weighted_accuracy   0.5962
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.17 ms/batch  | loss  0.17
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9809
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9719
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9661
| end of epoch   1 | time: 22.07s | weighted_accuracy   0.9752
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9809
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9719
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9661
| end of training | weighted_accuracy   0.9752
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 55,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8359999126858182,time reward:11.13574590909091
--------->Reward: 11.971745821776727
==========================================================================================
=-=-=-=-=-=>Episode: 55 | Loss: [0.7146067] | LR: (0.495, 56) | Mean R: 11.971745821776727 | Reward: [3.4783325966103575]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 56<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8611%
nonzero parameters after pruning: 5054 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23608 / 44426 (46.8599%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9935%
nonzero parameters after pruning: 6146 / 30720

[at weight fc2.weight]
percentage of pruned: 80.3175%
nonzero parameters after pruning: 1984 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11083 / 44426 (75.0529%)
-----------------------------------------------------------------------------------------
latency_list: [12.848008714285713, 10.75125165, 8.888910562500001]
runs_number: [6833679.0, 9277606.0, 10792504.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7569
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4122
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1431
| test before training | weighted_accuracy   0.5307
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.26 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9739
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9740
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9609
| end of epoch   1 | time: 22.09s | weighted_accuracy   0.9713
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9739
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9740
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9609
| end of training | weighted_accuracy   0.9713
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 56,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7925554116566976,time reward:11.183540454545454
--------->Reward: 11.476095866202153
==========================================================================================
=-=-=-=-=-=>Episode: 56 | Loss: [0.54328513] | LR: (0.495, 57) | Mean R: 11.476095866202153 | Reward: [2.9474299963556287]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 57<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9544%
nonzero parameters after pruning: 6158 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11158 / 44426 (74.8841%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.75125165, 8.970318374999998]
runs_number: [6832689.0, 9277606.0, 10694559.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8541
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4122
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1756
| test before training | weighted_accuracy   0.5858
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.02 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9765
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9744
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9685
| end of epoch   1 | time: 21.88s | weighted_accuracy   0.9743
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9765
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9744
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9685
| end of training | weighted_accuracy   0.9743
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 57,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8252227306365967,time reward:11.13857
--------->Reward: 11.963792730636596
==========================================================================================
=-=-=-=-=-=>Episode: 57 | Loss: [0.47588912] | LR: (0.495, 58) | Mean R: 11.963792730636596 | Reward: [3.395747661174644]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 58<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8611%
nonzero parameters after pruning: 5054 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23622 / 44426 (46.8284%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9479%
nonzero parameters after pruning: 6160 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11160 / 44426 (74.8796%)
-----------------------------------------------------------------------------------------
latency_list: [12.856692214285713, 10.75125165, 8.972489249999999]
runs_number: [6829063.0, 9277606.0, 10691972.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8086
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4122
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1725
| test before training | weighted_accuracy   0.5625
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.88 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9764
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9701
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9615
| end of epoch   1 | time: 21.72s | weighted_accuracy   0.9715
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9764
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9701
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9615
| end of training | weighted_accuracy   0.9715
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 58,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7947780026329888,time reward:11.13574590909091
--------->Reward: 11.930523911723897
==========================================================================================
=-=-=-=-=-=>Episode: 58 | Loss: [0.32929102] | LR: (0.495, 59) | Mean R: 11.930523911723897 | Reward: [3.324248205023503]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 59<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8115%
nonzero parameters after pruning: 5059 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23627 / 44426 (46.8172%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15274 / 44426 (65.6192%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0456%
nonzero parameters after pruning: 6130 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6925%
nonzero parameters after pruning: 2047 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11130 / 44426 (74.9471%)
-----------------------------------------------------------------------------------------
latency_list: [12.859793464285714, 10.7503833, 8.939926125]
runs_number: [6827416.0, 9278355.0, 10730917.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7891
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3683
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2839
| test before training | weighted_accuracy   0.5618
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.03 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9772
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9700
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9653
| end of epoch   1 | time: 21.89s | weighted_accuracy   0.9727
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9772
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9700
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9653
| end of training | weighted_accuracy   0.9727
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 59,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8073333899180095,time reward:11.153039999999999
--------->Reward: 11.960373389918008
==========================================================================================
=-=-=-=-=-=>Episode: 59 | Loss: [0.44571653] | LR: (0.495, 60) | Mean R: 11.960373389918008 | Reward: [3.3163716427342838]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 60<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23614 / 44426 (46.8464%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15274 / 44426 (65.6192%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0456%
nonzero parameters after pruning: 6130 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6429%
nonzero parameters after pruning: 2052 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11135 / 44426 (74.9358%)
-----------------------------------------------------------------------------------------
latency_list: [12.851730214285713, 10.7503833, 8.945353312500002]
runs_number: [6831700.0, 9278355.0, 10724406.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7792
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3904
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2093
| test before training | weighted_accuracy   0.5486
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.96 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9703
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9681
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9619
| end of epoch   1 | time: 21.80s | weighted_accuracy   0.9680
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9703
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9681
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9619
| end of training | weighted_accuracy   0.9680
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 60,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7551111115349664,time reward:11.152027727272726
--------->Reward: 11.907138838807692
==========================================================================================
=-=-=-=-=-=>Episode: 60 | Loss: [0.28413188] | LR: (0.495, 61) | Mean R: 11.907138838807692 | Reward: [3.2266954963256396]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 61<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23614 / 44426 (46.8464%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15274 / 44426 (65.6192%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 80.0716%
nonzero parameters after pruning: 6122 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.1190%
nonzero parameters after pruning: 167 / 840

total pruning rate: 11124 / 44426 (74.9606%)
-----------------------------------------------------------------------------------------
latency_list: [12.851730214285713, 10.7503833, 8.933413500000002]
runs_number: [6831700.0, 9278355.0, 10738740.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7792
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2829
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2467
| test before training | weighted_accuracy   0.5238
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.30 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9788
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9761
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9649
| end of epoch   1 | time: 22.16s | weighted_accuracy   0.9752
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9788
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9761
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9649
| end of training | weighted_accuracy   0.9752
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 61,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8356667889489068,time reward:11.158543181818182
--------->Reward: 11.994209970767088
==========================================================================================
=-=-=-=-=-=>Episode: 61 | Loss: [0.26804936] | LR: (0.495, 62) | Mean R: 11.994209970767088 | Reward: [3.2772572563544955]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 62<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23614 / 44426 (46.8464%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15274 / 44426 (65.6192%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9479%
nonzero parameters after pruning: 6160 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11163 / 44426 (74.8728%)
-----------------------------------------------------------------------------------------
latency_list: [12.851730214285713, 10.7503833, 8.975745562500002]
runs_number: [6831700.0, 9278355.0, 10688093.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7792
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2829
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1074
| test before training | weighted_accuracy   0.4959
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.00 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9763
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9744
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9660
| end of epoch   1 | time: 21.87s | weighted_accuracy   0.9737
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9763
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9744
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9660
| end of training | weighted_accuracy   0.9737
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 62,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8185556199815539,time reward:11.135521818181818
--------->Reward: 11.954077438163372
==========================================================================================
=-=-=-=-=-=>Episode: 62 | Loss: [0.21240301] | LR: (0.495, 63) | Mean R: 11.954077438163372 | Reward: [3.201726766711154]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 63<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0065%
nonzero parameters after pruning: 15358 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23614 / 44426 (46.8464%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15274 / 44426 (65.6192%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9479%
nonzero parameters after pruning: 6160 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11163 / 44426 (74.8728%)
-----------------------------------------------------------------------------------------
latency_list: [12.851730214285713, 10.7503833, 8.975745562500002]
runs_number: [6831700.0, 9278355.0, 10688093.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7792
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2829
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1074
| test before training | weighted_accuracy   0.4959
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.82 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9740
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9697
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9651
| end of epoch   1 | time: 21.66s | weighted_accuracy   0.9709
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9740
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9697
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9651
| end of training | weighted_accuracy   0.9709
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 63,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.788110891977946,time reward:11.135521818181818
--------->Reward: 11.923632710159763
==========================================================================================
=-=-=-=-=-=>Episode: 63 | Loss: [0.1760166] | LR: (0.495, 64) | Mean R: 11.923632710159763 | Reward: [3.136876272667097]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 64<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23600 / 44426 (46.8780%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15274 / 44426 (65.6192%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9479%
nonzero parameters after pruning: 6160 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11163 / 44426 (74.8728%)
-----------------------------------------------------------------------------------------
latency_list: [12.843046714285713, 10.7503833, 8.975745562500002]
runs_number: [6836319.0, 9278355.0, 10688093.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6679
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2829
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1074
| test before training | weighted_accuracy   0.4403
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.96 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9738
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9710
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9595
| end of epoch   1 | time: 21.82s | weighted_accuracy   0.9701
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9738
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9710
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9595
| end of training | weighted_accuracy   0.9701
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 64,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7788893911573622,time reward:11.137621363636363
--------->Reward: 11.916510754793725
==========================================================================================
=-=-=-=-=-=>Episode: 64 | Loss: [0.09356948] | LR: (0.495, 65) | Mean R: 11.916510754793725 | Reward: [3.096104592392072]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 65<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23600 / 44426 (46.8780%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.8810%
nonzero parameters after pruning: 253 / 840

total pruning rate: 15274 / 44426 (65.6192%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9479%
nonzero parameters after pruning: 6160 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11163 / 44426 (74.8728%)
-----------------------------------------------------------------------------------------
latency_list: [12.843046714285713, 10.7503833, 8.975745562500002]
runs_number: [6836319.0, 9278355.0, 10688093.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6679
| test_data evaluate | sub loss  0.01 | sub accuracy   0.2829
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1074
| test before training | weighted_accuracy   0.4403
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.20 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9703
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9669
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9615
| end of epoch   1 | time: 22.01s | weighted_accuracy   0.9675
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9703
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9669
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9615
| end of training | weighted_accuracy   0.9675
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 65,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7502222061157227,time reward:11.137621363636363
--------->Reward: 11.887843569752086
==========================================================================================
=-=-=-=-=-=>Episode: 65 | Loss: [0.10857081] | LR: (0.495, 66) | Mean R: 11.887843569752086 | Reward: [3.0347129895461915]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 66<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23600 / 44426 (46.8780%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9479%
nonzero parameters after pruning: 6160 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11163 / 44426 (74.8728%)
-----------------------------------------------------------------------------------------
latency_list: [12.843046714285713, 10.75125165, 8.975745562500002]
runs_number: [6836319.0, 9277606.0, 10688093.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6679
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4151
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1074
| test before training | weighted_accuracy   0.4800
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 20.02 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9715
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9673
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9602
| end of epoch   1 | time: 22.83s | weighted_accuracy   0.9680
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9715
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9673
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9602
| end of training | weighted_accuracy   0.9680
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 66,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7553336355421278,time reward:11.137280909090908
--------->Reward: 11.892614544633036
==========================================================================================
=-=-=-=-=-=>Episode: 66 | Loss: [0.16835134] | LR: (0.495, 67) | Mean R: 11.892614544633036 | Reward: [3.0073692449449414]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 67<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.9405%
nonzero parameters after pruning: 5046 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23600 / 44426 (46.8780%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9479%
nonzero parameters after pruning: 6160 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11163 / 44426 (74.8728%)
-----------------------------------------------------------------------------------------
latency_list: [12.843046714285713, 10.75125165, 8.975745562500002]
runs_number: [6836319.0, 9277606.0, 10688093.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.6679
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4151
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1074
| test before training | weighted_accuracy   0.4800
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.40 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9750
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9651
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9595
| end of epoch   1 | time: 22.24s | weighted_accuracy   0.9689
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9750
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9651
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9595
| end of training | weighted_accuracy   0.9689
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 67,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7658889558580186,time reward:11.137280909090908
--------->Reward: 11.903169864948927
==========================================================================================
=-=-=-=-=-=>Episode: 67 | Loss: [0.06379235] | LR: (0.495, 68) | Mean R: 11.903169864948927 | Reward: [2.986353850617455]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 68<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9479%
nonzero parameters after pruning: 6160 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11163 / 44426 (74.8728%)
-----------------------------------------------------------------------------------------
latency_list: [12.850489714285713, 10.75125165, 8.975745562500002]
runs_number: [6832359.0, 9277606.0, 10688093.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7668
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4151
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1074
| test before training | weighted_accuracy   0.5294
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.63 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9739
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9563
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9578
| end of epoch   1 | time: 22.51s | weighted_accuracy   0.9654
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9739
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9563
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9578
| end of training | weighted_accuracy   0.9654
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 68,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7266664505004883,time reward:11.135480909090909
--------->Reward: 11.362147359591397
==========================================================================================
=-=-=-=-=-=>Episode: 68 | Loss: [0.08516575] | LR: (0.495, 69) | Mean R: 11.362147359591397 | Reward: [2.4189369267438803]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 69<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8413%
nonzero parameters after pruning: 5056 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23610 / 44426 (46.8554%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9870%
nonzero parameters after pruning: 6148 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11151 / 44426 (74.8998%)
-----------------------------------------------------------------------------------------
latency_list: [12.849249214285713, 10.75125165, 8.9627203125]
runs_number: [6833019.0, 9277606.0, 10703626.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7933
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4151
| test_data evaluate | sub loss  0.01 | sub accuracy   0.0964
| test before training | weighted_accuracy   0.5405
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.65 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9745
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9700
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9668
| end of epoch   1 | time: 22.58s | weighted_accuracy   0.9716
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9745
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9700
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9668
| end of training | weighted_accuracy   0.9716
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 69,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7956661118401421,time reward:11.142841363636363
--------->Reward: 11.938507475476504
==========================================================================================
=-=-=-=-=-=>Episode: 69 | Loss: [0.08122097] | LR: (0.495, 70) | Mean R: 11.938507475476504 | Reward: [2.964654092282027]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 70<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8413%
nonzero parameters after pruning: 5056 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23610 / 44426 (46.8554%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9870%
nonzero parameters after pruning: 6148 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11151 / 44426 (74.8998%)
-----------------------------------------------------------------------------------------
latency_list: [12.849249214285713, 10.75125165, 8.9627203125]
runs_number: [6833019.0, 9277606.0, 10703626.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7933
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4151
| test_data evaluate | sub loss  0.01 | sub accuracy   0.0964
| test before training | weighted_accuracy   0.5405
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 20.04 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9679
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9645
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9608
| end of epoch   1 | time: 23.12s | weighted_accuracy   0.9655
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9679
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9645
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9608
| end of training | weighted_accuracy   0.9655
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 70,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7273333602481418,time reward:11.142841363636363
--------->Reward: 11.870174723884505
==========================================================================================
=-=-=-=-=-=>Episode: 70 | Loss: [0.04793799] | LR: (0.495, 71) | Mean R: 11.870174723884505 | Reward: [2.866827083416643]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 71<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8413%
nonzero parameters after pruning: 5056 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23610 / 44426 (46.8554%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9870%
nonzero parameters after pruning: 6148 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11151 / 44426 (74.8998%)
-----------------------------------------------------------------------------------------
latency_list: [12.849249214285713, 10.75125165, 8.9627203125]
runs_number: [6833019.0, 9277606.0, 10703626.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7933
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4151
| test_data evaluate | sub loss  0.01 | sub accuracy   0.0964
| test before training | weighted_accuracy   0.5405
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 20.04 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9767
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9734
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9676
| end of epoch   1 | time: 22.91s | weighted_accuracy   0.9739
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9767
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9734
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9676
| end of training | weighted_accuracy   0.9739
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 71,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.142841363636363
--------->Reward: 11.963841436327538
==========================================================================================
=-=-=-=-=-=>Episode: 71 | Loss: [0.05817974] | LR: (0.495, 72) | Mean R: 11.963841436327538 | Reward: [2.9308093305188496]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 72<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8413%
nonzero parameters after pruning: 5056 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23610 / 44426 (46.8554%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9870%
nonzero parameters after pruning: 6148 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11151 / 44426 (74.8998%)
-----------------------------------------------------------------------------------------
latency_list: [12.849249214285713, 10.75125165, 8.9627203125]
runs_number: [6833019.0, 9277606.0, 10703626.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7933
| test_data evaluate | sub loss  0.00 | sub accuracy   0.4151
| test_data evaluate | sub loss  0.01 | sub accuracy   0.0964
| test before training | weighted_accuracy   0.5405
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.14 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9769
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9695
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9664
| end of epoch   1 | time: 21.96s | weighted_accuracy   0.9726
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9769
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9695
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9664
| end of training | weighted_accuracy   0.9726
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 72,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8064439561631945,time reward:11.142841363636363
--------->Reward: 11.949285319799557
==========================================================================================
=-=-=-=-=-=>Episode: 72 | Loss: [0.03426153] | LR: (0.495, 73) | Mean R: 11.949285319799557 | Reward: [2.8872460037004792]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 73<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8413%
nonzero parameters after pruning: 5056 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23610 / 44426 (46.8554%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.7024%
nonzero parameters after pruning: 3054 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15274 / 44426 (65.6192%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11157 / 44426 (74.8863%)
-----------------------------------------------------------------------------------------
latency_list: [12.849249214285713, 10.7503833, 8.9692329375]
runs_number: [6833019.0, 9278355.0, 10695854.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7933
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3352
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1457
| test before training | weighted_accuracy   0.5264
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.91 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9778
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9740
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9612
| end of epoch   1 | time: 21.78s | weighted_accuracy   0.9733
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9778
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9740
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9612
| end of training | weighted_accuracy   0.9733
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 73,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8148886097802056,time reward:11.13964909090909
--------->Reward: 11.954537700689295
==========================================================================================
=-=-=-=-=-=>Episode: 73 | Loss: [0.21372744] | LR: (0.495, 74) | Mean R: 11.954537700689295 | Reward: [2.8639950775397924]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 74<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8413%
nonzero parameters after pruning: 5056 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23610 / 44426 (46.8554%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11157 / 44426 (74.8863%)
-----------------------------------------------------------------------------------------
latency_list: [12.849249214285713, 10.75125165, 8.9692329375]
runs_number: [6833019.0, 9277606.0, 10695854.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7933
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3886
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1457
| test before training | weighted_accuracy   0.5424
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.12 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9777
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9755
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9654
| end of epoch   1 | time: 22.00s | weighted_accuracy   0.9746
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9777
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9755
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9654
| end of training | weighted_accuracy   0.9746
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 74,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8286665545569526,time reward:11.139308636363635
--------->Reward: 11.967975190920589
==========================================================================================
=-=-=-=-=-=>Episode: 74 | Loss: [0.06920833] | LR: (0.495, 75) | Mean R: 11.967975190920589 | Reward: [2.849356648372982]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 75<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 75,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8286665545569526,time reward:11.139308636363635
--------->Reward: 11.967975190920589
==========================================================================================
=-=-=-=-=-=>Episode: 75 | Loss: [0.0141252] | LR: (0.495, 76) | Mean R: 11.967975190920589 | Reward: [2.8217998846618784]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 76<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11157 / 44426 (74.8863%)
-----------------------------------------------------------------------------------------
latency_list: [12.850489714285713, 10.75125165, 8.9692329375]
runs_number: [6832359.0, 9277606.0, 10695854.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7106
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3886
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1457
| test before training | weighted_accuracy   0.5010
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.86 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9716
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9710
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9647
| end of epoch   1 | time: 21.73s | weighted_accuracy   0.9700
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9716
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9710
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9647
| end of training | weighted_accuracy   0.9700
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 76,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7782218191358778,time reward:11.139008636363636
--------->Reward: 11.917230455499514
==========================================================================================
=-=-=-=-=-=>Episode: 76 | Loss: [0.03008296] | LR: (0.495, 77) | Mean R: 11.917230455499514 | Reward: [2.7443873175537554]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 77<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11157 / 44426 (74.8863%)
-----------------------------------------------------------------------------------------
latency_list: [12.850489714285713, 10.75125165, 8.9692329375]
runs_number: [6832359.0, 9277606.0, 10695854.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7106
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3886
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1457
| test before training | weighted_accuracy   0.5010
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.20 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9749
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9655
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9623
| end of epoch   1 | time: 22.12s | weighted_accuracy   0.9696
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9749
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9655
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9623
| end of training | weighted_accuracy   0.9696
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 77,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.772888527976142,time reward:11.139008636363636
--------->Reward: 11.911897164339779
==========================================================================================
=-=-=-=-=-=>Episode: 77 | Loss: [0.05780875] | LR: (0.495, 78) | Mean R: 11.911897164339779 | Reward: [2.712913943636215]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 78<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 78,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.772888527976142,time reward:11.139008636363636
--------->Reward: 11.911897164339779
==========================================================================================
=-=-=-=-=-=>Episode: 78 | Loss: [0.01110355] | LR: (0.495, 79) | Mean R: 11.911897164339779 | Reward: [2.687248868335793]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 79<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11157 / 44426 (74.8863%)
-----------------------------------------------------------------------------------------
latency_list: [12.850489714285713, 10.75125165, 8.9692329375]
runs_number: [6832359.0, 9277606.0, 10695854.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7106
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3886
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1174
| test before training | weighted_accuracy   0.4954
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.07 ms/batch  | loss  0.21
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9698
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9695
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9581
| end of epoch   1 | time: 21.93s | weighted_accuracy   0.9674
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9698
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9695
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9581
| end of training | weighted_accuracy   0.9674
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 79,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.748555925157335,time reward:11.139008636363636
--------->Reward: 11.887564561520971
==========================================================================================
=-=-=-=-=-=>Episode: 79 | Loss: [0.10550082] | LR: (0.495, 80) | Mean R: 11.887564561520971 | Reward: [2.6378909224785136]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 80<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11157 / 44426 (74.8863%)
-----------------------------------------------------------------------------------------
latency_list: [12.850489714285713, 10.75125165, 8.9692329375]
runs_number: [6832359.0, 9277606.0, 10695854.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7106
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3886
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1174
| test before training | weighted_accuracy   0.4954
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.35 ms/batch  | loss  0.21
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9735
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9698
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9597
| end of epoch   1 | time: 22.29s | weighted_accuracy   0.9696
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9735
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9698
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9597
| end of training | weighted_accuracy   0.9696
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 80,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7736666997273763,time reward:11.139008636363636
--------->Reward: 11.912675336091013
==========================================================================================
=-=-=-=-=-=>Episode: 80 | Loss: [0.03656361] | LR: (0.495, 81) | Mean R: 11.912675336091013 | Reward: [2.6382451247823155]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 81<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11157 / 44426 (74.8863%)
-----------------------------------------------------------------------------------------
latency_list: [12.850489714285713, 10.79987925, 8.9692329375]
runs_number: [6832359.0, 9235832.0, 10695854.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7106
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1174
| test before training | weighted_accuracy   0.4631
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.91 ms/batch  | loss  0.21
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9717
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9723
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9626
| end of epoch   1 | time: 21.82s | weighted_accuracy   0.9701
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9717
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9723
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9626
| end of training | weighted_accuracy   0.9701
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 81,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7784443431430393,time reward:11.120020454545454
--------->Reward: 11.398464797688494
==========================================================================================
=-=-=-=-=-=>Episode: 81 | Loss: [0.08886536] | LR: (0.495, 82) | Mean R: 11.398464797688494 | Reward: [2.103363652041555]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 82<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 82,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7784443431430393,time reward:11.120020454545454
--------->Reward: 11.398464797688494
==========================================================================================
=-=-=-=-=-=>Episode: 82 | Loss: [0.01124952] | LR: (0.495, 83) | Mean R: 11.398464797688494 | Reward: [2.083077957337096]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 83<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11157 / 44426 (74.8863%)
-----------------------------------------------------------------------------------------
latency_list: [12.850489714285713, 10.79987925, 8.9692329375]
runs_number: [6832359.0, 9235832.0, 10695854.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7106
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1174
| test before training | weighted_accuracy   0.4631
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.10 ms/batch  | loss  0.20
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9788
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9744
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9644
| end of epoch   1 | time: 22.04s | weighted_accuracy   0.9746
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9788
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9744
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9644
| end of training | weighted_accuracy   0.9746
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 83,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8288897408379449,time reward:11.120020454545454
--------->Reward: 11.948910195383398
==========================================================================================
=-=-=-=-=-=>Episode: 83 | Loss: [0.03940928] | LR: (0.495, 84) | Mean R: 11.948910195383398 | Reward: [2.609802814406219]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 84<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.0000%
nonzero parameters after pruning: 168 / 840

total pruning rate: 11157 / 44426 (74.8863%)
-----------------------------------------------------------------------------------------
latency_list: [12.850489714285713, 10.79987925, 8.9692329375]
runs_number: [6832359.0, 9235832.0, 10695854.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7106
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1174
| test before training | weighted_accuracy   0.4631
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.08 ms/batch  | loss  0.21
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9756
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9756
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9644
| end of epoch   1 | time: 21.98s | weighted_accuracy   0.9734
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9756
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9756
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9644
| end of training | weighted_accuracy   0.9734
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 84,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.815111796061198,time reward:11.120020454545454
--------->Reward: 11.435132250606651
==========================================================================================
=-=-=-=-=-=>Episode: 84 | Loss: [0.04506571] | LR: (0.495, 85) | Mean R: 11.435132250606651 | Reward: [2.0762282618742223]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 85<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 85,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8288897408379449,time reward:11.120020454545454
--------->Reward: 11.948910195383398
==========================================================================================
=-=-=-=-=-=>Episode: 85 | Loss: [0.01572942] | LR: (0.495, 86) | Mean R: 11.948910195383398 | Reward: [2.56709518658762]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 86<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.850489714285713, 10.79987925, 8.965976625]
runs_number: [6832359.0, 9235832.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7106
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.4774
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 18.96 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9781
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9720
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9646
| end of epoch   1 | time: 21.86s | weighted_accuracy   0.9736
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9781
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9720
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9646
| end of training | weighted_accuracy   0.9736
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 86,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8174443244934082,time reward:11.121785909090908
--------->Reward: 11.939230233584317
==========================================================================================
=-=-=-=-=-=>Episode: 86 | Loss: [0.08907776] | LR: (0.495, 87) | Mean R: 11.939230233584317 | Reward: [2.53495827989485]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 87<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8214%
nonzero parameters after pruning: 5058 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23612 / 44426 (46.8509%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.850489714285713, 10.79987925, 8.965976625]
runs_number: [6832359.0, 9235832.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.7106
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.4774
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.62 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9783
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9687
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9651
| end of epoch   1 | time: 22.54s | weighted_accuracy   0.9728
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9783
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9687
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9651
| end of training | weighted_accuracy   0.9728
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 87,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8086665471394857,time reward:11.121785909090908
--------->Reward: 11.930452456230395
==========================================================================================
=-=-=-=-=-=>Episode: 87 | Loss: [0.01455595] | LR: (0.495, 88) | Mean R: 11.930452456230395 | Reward: [2.504160790768678]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 88<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 88,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8086665471394857,time reward:11.121785909090908
--------->Reward: 11.930452456230395
==========================================================================================
=-=-=-=-=-=>Episode: 88 | Loss: [0.00728159] | LR: (0.495, 89) | Mean R: 11.930452456230395 | Reward: [2.482510541293726]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 89<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.79987925, 8.965976625]
runs_number: [6832689.0, 9235832.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8045
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.5243
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.21 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9752
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9616
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9625
| end of epoch   1 | time: 22.06s | weighted_accuracy   0.9686
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9752
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9616
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9625
| end of training | weighted_accuracy   0.9686
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 89,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7620000839233398,time reward:11.121935909090908
--------->Reward: 11.383935993014248
==========================================================================================
=-=-=-=-=-=>Episode: 89 | Loss: [0.05885074] | LR: (0.495, 90) | Mean R: 11.383935993014248 | Reward: [1.918235187564152]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 90<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.79987925, 8.965976625]
runs_number: [6832689.0, 9235832.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8045
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.5243
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.34 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9753
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9691
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9625
| end of epoch   1 | time: 22.29s | weighted_accuracy   0.9709
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9753
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9691
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9625
| end of training | weighted_accuracy   0.9709
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 90,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7875552442338731,time reward:11.121935909090908
--------->Reward: 11.909491153324781
==========================================================================================
=-=-=-=-=-=>Episode: 90 | Loss: [0.07128539] | LR: (0.495, 91) | Mean R: 11.909491153324781 | Reward: [2.4229869248195772]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 91<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 91,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7875552442338731,time reward:11.121935909090908
--------->Reward: 11.909491153324781
==========================================================================================
=-=-=-=-=-=>Episode: 91 | Loss: [0.00766523] | LR: (0.495, 92) | Mean R: 11.909491153324781 | Reward: [2.4025253169917455]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 92<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.79987925, 8.965976625]
runs_number: [6832689.0, 9235832.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8045
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.5243
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.31 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9723
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9685
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9613
| end of epoch   1 | time: 22.22s | weighted_accuracy   0.9690
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9723
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9685
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9613
| end of training | weighted_accuracy   0.9690
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 92,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7662227418687608,time reward:11.121935909090908
--------->Reward: 11.888158650959669
==========================================================================================
=-=-=-=-=-=>Episode: 92 | Loss: [0.02059497] | LR: (0.495, 93) | Mean R: 11.888158650959669 | Reward: [2.361198347102338]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 93<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 93,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7662227418687608,time reward:11.121935909090908
--------->Reward: 11.888158650959669
==========================================================================================
=-=-=-=-=-=>Episode: 93 | Loss: [0.00426891] | LR: (0.495, 94) | Mean R: 11.888158650959669 | Reward: [2.341528421808377]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 94<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.79987925, 8.965976625]
runs_number: [6832689.0, 9235832.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8045
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.5243
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.45 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9794
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9720
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9674
| end of epoch   1 | time: 22.38s | weighted_accuracy   0.9748
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9794
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9720
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9674
| end of training | weighted_accuracy   0.9748
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 94,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8308891455332439,time reward:11.121935909090908
--------->Reward: 11.952825054624151
==========================================================================================
=-=-=-=-=-=>Episode: 94 | Loss: [0.00510268] | LR: (0.495, 95) | Mean R: 11.952825054624151 | Reward: [2.3864460669161964]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 95<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.79987925, 8.965976625]
runs_number: [6832689.0, 9235832.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8045
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.5243
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.38 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9759
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9716
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9671
| end of epoch   1 | time: 22.25s | weighted_accuracy   0.9729
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9759
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9716
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9671
| end of training | weighted_accuracy   0.9729
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 95,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.80944471889072,time reward:11.121935909090908
--------->Reward: 11.931380627981628
==========================================================================================
=-=-=-=-=-=>Episode: 95 | Loss: [0.02622413] | LR: (0.495, 96) | Mean R: 11.931380627981628 | Reward: [2.3456964899793284]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 96<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 96,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.80944471889072,time reward:11.121935909090908
--------->Reward: 11.931380627981628
==========================================================================================
=-=-=-=-=-=>Episode: 96 | Loss: [0.00323297] | LR: (0.495, 97) | Mean R: 11.931380627981628 | Reward: [2.326696555453548]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 97<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 69.9349%
nonzero parameters after pruning: 9236 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15331 / 44426 (65.4909%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.79987925, 8.965976625]
runs_number: [6832689.0, 9235832.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8045
| test_data evaluate | sub loss  0.00 | sub accuracy   0.2811
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.5243
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.08 ms/batch  | loss  0.18
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9781
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9748
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9685
| end of epoch   1 | time: 21.98s | weighted_accuracy   0.9752
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9781
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9748
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9685
| end of training | weighted_accuracy   0.9752
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 97,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8354442649417454,time reward:11.121935909090908
--------->Reward: 11.957380174032654
==========================================================================================
=-=-=-=-=-=>Episode: 97 | Loss: [0.07414865] | LR: (0.495, 98) | Mean R: 11.957380174032654 | Reward: [2.3338401603799515]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 98<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 98,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8354442649417454,time reward:11.121935909090908
--------->Reward: 11.957380174032654
==========================================================================================
=-=-=-=-=-=>Episode: 98 | Loss: [0.00686146] | LR: (0.495, 99) | Mean R: 11.957380174032654 | Reward: [2.315277275879108]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 99<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 99,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8354442649417454,time reward:11.121935909090908
--------->Reward: 11.957380174032654
==========================================================================================
=-=-=-=-=-=>Episode: 99 | Loss: [0.00366231] | LR: (0.2475, 100) | Mean R: 11.957380174032654 | Reward: [2.2970008734531113]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 100<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
************Process:********** 33.33333333333333%
====================Results=======================
--------->Episode: 100,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8354442649417454,time reward:11.121935909090908
--------->Reward: 11.957380174032654
==========================================================================================
=-=-=-=-=-=>Episode: 100 | Loss: [0.0032673] | LR: (0.2475, 101) | Mean R: 11.957380174032654 | Reward: [2.2790045654321744]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 101<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 101,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8354442649417454,time reward:11.121935909090908
--------->Reward: 11.957380174032654
==========================================================================================
=-=-=-=-=-=>Episode: 101 | Loss: [0.00294853] | LR: (0.2475, 102) | Mean R: 11.957380174032654 | Reward: [2.2612821448632268]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 102<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 102,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8354442649417454,time reward:11.121935909090908
--------->Reward: 11.957380174032654
==========================================================================================
=-=-=-=-=-=>Episode: 102 | Loss: [0.00268488] | LR: (0.2475, 103) | Mean R: 11.957380174032654 | Reward: [2.2438275795621685]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 103<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 103,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8354442649417454,time reward:11.121935909090908
--------->Reward: 11.957380174032654
==========================================================================================
=-=-=-=-=-=>Episode: 103 | Loss: [0.00245839] | LR: (0.2475, 104) | Mean R: 11.957380174032654 | Reward: [2.2266350063742983]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 104<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 104,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8354442649417454,time reward:11.121935909090908
--------->Reward: 11.957380174032654
==========================================================================================
=-=-=-=-=-=>Episode: 104 | Loss: [0.00226471] | LR: (0.2475, 105) | Mean R: 11.957380174032654 | Reward: [2.209698725636345]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 105<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 105,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8354442649417454,time reward:11.121935909090908
--------->Reward: 11.957380174032654
==========================================================================================
=-=-=-=-=-=>Episode: 105 | Loss: [0.00209628] | LR: (0.2475, 106) | Mean R: 11.957380174032654 | Reward: [2.193013195833787]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 106<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 106,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8354442649417454,time reward:11.121935909090908
--------->Reward: 11.957380174032654
==========================================================================================
=-=-=-=-=-=>Episode: 106 | Loss: [0.0019478] | LR: (0.2475, 107) | Mean R: 11.957380174032654 | Reward: [2.1765730284469704]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 107<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1139%
nonzero parameters after pruning: 9181 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15276 / 44426 (65.6147%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.752120000000001, 8.965976625]
runs_number: [6832689.0, 9276857.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8045
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3725
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.5518
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.42 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9752
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9729
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9640
| end of epoch   1 | time: 22.39s | weighted_accuracy   0.9723
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9752
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9729
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9640
| end of training | weighted_accuracy   0.9723
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 107,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8029994699690077,time reward:11.140583636363635
--------->Reward: 11.943583106332643
==========================================================================================
=-=-=-=-=-=>Episode: 107 | Loss: [0.07405308] | LR: (0.2475, 108) | Mean R: 11.943583106332643 | Reward: [2.1466502062430823]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 108<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.75125165, 8.965976625]
runs_number: [6832689.0, 9277606.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8045
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3886
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.5566
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.70 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9731
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9680
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9611
| end of epoch   1 | time: 22.61s | weighted_accuracy   0.9692
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9731
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9680
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9611
| end of training | weighted_accuracy   0.9692
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 108,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7685552703009711,time reward:11.14092409090909
--------->Reward: 11.909479361210062
==========================================================================================
=-=-=-=-=-=>Episode: 108 | Loss: [0.0124558] | LR: (0.2475, 109) | Mean R: 11.909479361210062 | Reward: [2.0968370037236213]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 109<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 109,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7685552703009711,time reward:11.14092409090909
--------->Reward: 11.909479361210062
==========================================================================================
=-=-=-=-=-=>Episode: 109 | Loss: [0.00369195] | LR: (0.2475, 110) | Mean R: 11.909479361210062 | Reward: [2.0813552968979074]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 110<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 110,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.7685552703009711,time reward:11.14092409090909
--------->Reward: 11.909479361210062
==========================================================================================
=-=-=-=-=-=>Episode: 110 | Loss: [0.00311236] | LR: (0.2475, 111) | Mean R: 11.909479361210062 | Reward: [2.066096536275488]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 111<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.75125165, 8.965976625]
runs_number: [6832689.0, 9277606.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8045
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3886
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.5566
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 20.10 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9767
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9697
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9648
| end of epoch   1 | time: 22.94s | weighted_accuracy   0.9722
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9767
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9697
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9648
| end of training | weighted_accuracy   0.9722
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 111,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 111 | Loss: [0.0042934] | LR: (0.2475, 112) | Mean R: 11.943368575407856 | Reward: [2.0847692959139295]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 112<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 112,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 112 | Loss: [0.00259096] | LR: (0.2475, 113) | Mean R: 11.943368575407856 | Reward: [2.0697681128319]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 113<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 113,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 113 | Loss: [0.00225945] | LR: (0.2475, 114) | Mean R: 11.943368575407856 | Reward: [2.0549777616935536]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 114<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 114,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 114 | Loss: [0.00200845] | LR: (0.2475, 115) | Mean R: 11.943368575407856 | Reward: [2.040393920782323]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 115<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 115,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 115 | Loss: [0.00180754] | LR: (0.2475, 116) | Mean R: 11.943368575407856 | Reward: [2.0260123824695455]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 116<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 116,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 116 | Loss: [0.00164129] | LR: (0.2475, 117) | Mean R: 11.943368575407856 | Reward: [2.0118290496176012]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 117<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 117,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 117 | Loss: [0.00150019] | LR: (0.2475, 118) | Mean R: 11.943368575407856 | Reward: [1.9978399321105371]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 118<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 118,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 118 | Loss: [0.0013785] | LR: (0.2475, 119) | Mean R: 11.943368575407856 | Reward: [1.984041143507433]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 119<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 119,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 119 | Loss: [0.00127252] | LR: (0.2475, 120) | Mean R: 11.943368575407856 | Reward: [1.9704288978139566]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 120<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 120,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 120 | Loss: [0.00117858] | LR: (0.2475, 121) | Mean R: 11.943368575407856 | Reward: [1.9569995063675467]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 121<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 121,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 121 | Loss: [0.00109384] | LR: (0.2475, 122) | Mean R: 11.943368575407856 | Reward: [1.9437493748322368]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 122<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 122,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 122 | Loss: [0.00101856] | LR: (0.2475, 123) | Mean R: 11.943368575407856 | Reward: [1.930675000298713]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 123<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 123,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8024444844987657,time reward:11.14092409090909
--------->Reward: 11.943368575407856
==========================================================================================
=-=-=-=-=-=>Episode: 123 | Loss: [0.00095098] | LR: (0.2475, 124) | Mean R: 11.943368575407856 | Reward: [1.9177729684858988]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 124<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
[at weight fc1.weight]
percentage of pruned: 50.0521%
nonzero parameters after pruning: 15344 / 30720

[at weight fc2.weight]
percentage of pruned: 49.8313%
nonzero parameters after pruning: 5057 / 10080

[at weight fc3.weight]
percentage of pruned: 49.5238%
nonzero parameters after pruning: 424 / 840

total pruning rate: 23611 / 44426 (46.8532%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 70.1172%
nonzero parameters after pruning: 9180 / 30720

[at weight fc2.weight]
percentage of pruned: 69.6925%
nonzero parameters after pruning: 3055 / 10080

[at weight fc3.weight]
percentage of pruned: 69.7619%
nonzero parameters after pruning: 254 / 840

total pruning rate: 15275 / 44426 (65.6170%)
-----------------------------------------------------------------------------------------
[at weight fc1.weight]
percentage of pruned: 79.9674%
nonzero parameters after pruning: 6154 / 30720

[at weight fc2.weight]
percentage of pruned: 79.6726%
nonzero parameters after pruning: 2049 / 10080

[at weight fc3.weight]
percentage of pruned: 80.3571%
nonzero parameters after pruning: 165 / 840

total pruning rate: 11154 / 44426 (74.8931%)
-----------------------------------------------------------------------------------------
latency_list: [12.849869464285714, 10.75125165, 8.965976625]
runs_number: [6832689.0, 9277606.0, 10699738.0]
-----------------------------------------------------------------------------------------
| original model evaluate | loss  0.00 | accuracy   0.9865
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.8045
| test_data evaluate | sub loss  0.00 | sub accuracy   0.3886
| test_data evaluate | sub loss  0.01 | sub accuracy   0.1888
| test before training | weighted_accuracy   0.5566
-----------------------------------------------------------------------------------------
| epoch   1 |   900/  938 batches | lr 0.23 | 19.44 ms/batch  | loss  0.19
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9758
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9741
| end of epoch   1 | sub loss  0.00 | sub accuracy   0.9688
| end of epoch   1 | time: 22.82s | weighted_accuracy   0.9739
-----------------------------------------------------------------------------------------
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9758
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9741
| test_data evaluate | sub loss  0.00 | sub accuracy   0.9688
| end of training | weighted_accuracy   0.9739
-----------------------------------------------------------------------------------------
====================Results=======================
--------->Episode: 124,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 124 | Loss: [0.0580914] | LR: (0.2475, 125) | Mean R: 11.961924163600266 | Reward: [1.923509204954911]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 125<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 125,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 125 | Loss: [0.00510868] | LR: (0.2475, 126) | Mean R: 11.961924163600266 | Reward: [1.9108563041004505]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 126<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 126,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 126 | Loss: [0.00181277] | LR: (0.2475, 127) | Mean R: 11.961924163600266 | Reward: [1.8983666847170806]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 127<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 127,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 127 | Loss: [0.00148283] | LR: (0.2475, 128) | Mean R: 11.961924163600266 | Reward: [1.8860372557263059]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 128<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 128,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 128 | Loss: [0.00127417] | LR: (0.2475, 129) | Mean R: 11.961924163600266 | Reward: [1.8738650020301417]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 129<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 129,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 129 | Loss: [0.0011203] | LR: (0.2475, 130) | Mean R: 11.961924163600266 | Reward: [1.8618469822523185]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 130<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 130,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 130 | Loss: [0.00099955] | LR: (0.2475, 131) | Mean R: 11.961924163600266 | Reward: [1.8499803265567092]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 131<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 131,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 131 | Loss: [0.00090044] | LR: (0.2475, 132) | Mean R: 11.961924163600266 | Reward: [1.8382622345399735]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 132<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 132,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 132 | Loss: [0.00081806] | LR: (0.2475, 133) | Mean R: 11.961924163600266 | Reward: [1.826689973195677]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 133<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 133,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 133 | Loss: [0.00074756] | LR: (0.2475, 134) | Mean R: 11.961924163600266 | Reward: [1.8152608749472527]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 134<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 134,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 134 | Loss: [0.00068683] | LR: (0.2475, 135) | Mean R: 11.961924163600266 | Reward: [1.8039723357471686]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 135<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 135,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 135 | Loss: [0.00063327] | LR: (0.2475, 136) | Mean R: 11.961924163600266 | Reward: [1.792821813239918]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 136<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 136,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 136 | Loss: [0.00058613] | LR: (0.2475, 137) | Mean R: 11.961924163600266 | Reward: [1.781806824986388]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 137<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 137,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 137 | Loss: [0.00054443] | LR: (0.2475, 138) | Mean R: 11.961924163600266 | Reward: [1.7709249467473658]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 138<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 138,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 138 | Loss: [0.00050721] | LR: (0.2475, 139) | Mean R: 11.961924163600266 | Reward: [1.7601738108240461]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 139<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 139,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 139 | Loss: [0.00047367] | LR: (0.2475, 140) | Mean R: 11.961924163600266 | Reward: [1.7495511044533139]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 140<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 140,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 140 | Loss: [0.00044329] | LR: (0.2475, 141) | Mean R: 11.961924163600266 | Reward: [1.7390545682559235]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 141<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 141,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 141 | Loss: [0.00041562] | LR: (0.2475, 142) | Mean R: 11.961924163600266 | Reward: [1.728681994735524]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 142<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 142,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 142 | Loss: [0.00039028] | LR: (0.2475, 143) | Mean R: 11.961924163600266 | Reward: [1.7184312268267323]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 143<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 143,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 143 | Loss: [0.00036706] | LR: (0.2475, 144) | Mean R: 11.961924163600266 | Reward: [1.7083001564904734]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 144<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 144,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 144 | Loss: [0.00034559] | LR: (0.2475, 145) | Mean R: 11.961924163600266 | Reward: [1.6982867233547854]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 145<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 145,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 145 | Loss: [0.00032578] | LR: (0.2475, 146) | Mean R: 11.961924163600266 | Reward: [1.6883889133995211]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 146<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 146,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 146 | Loss: [0.00030748] | LR: (0.2475, 147) | Mean R: 11.961924163600266 | Reward: [1.6786047576833436]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 147<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 147,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 147 | Loss: [0.0002905] | LR: (0.2475, 148) | Mean R: 11.961924163600266 | Reward: [1.6689323311114563]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 148<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 148,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 148 | Loss: [0.00027466] | LR: (0.2475, 149) | Mean R: 11.961924163600266 | Reward: [1.659369751242597]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 149<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 149,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 149 | Loss: [0.00025977] | LR: (0.2475, 150) | Mean R: 11.961924163600266 | Reward: [1.6499151771339893]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 150<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
************Process:********** 50.0%
====================Results=======================
--------->Episode: 150,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 150 | Loss: [0.00024589] | LR: (0.12375, 151) | Mean R: 11.961924163600266 | Reward: [1.6405668082227152]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 151<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 151,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 151 | Loss: [0.00023929] | LR: (0.12375, 152) | Mean R: 11.961924163600266 | Reward: [1.6313228832424098]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 152<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 152,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 152 | Loss: [0.00023275] | LR: (0.12375, 153) | Mean R: 11.961924163600266 | Reward: [1.6221816791738526]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 153<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 153,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 153 | Loss: [0.00022627] | LR: (0.12375, 154) | Mean R: 11.961924163600266 | Reward: [1.613141510228381]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 154<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 154,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 154 | Loss: [0.00021982] | LR: (0.12375, 155) | Mean R: 11.961924163600266 | Reward: [1.6042007268628868]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 155<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 155,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 155 | Loss: [0.00021344] | LR: (0.12375, 156) | Mean R: 11.961924163600266 | Reward: [1.5953577148253526]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 156<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 156,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 156 | Loss: [0.00020715] | LR: (0.12375, 157) | Mean R: 11.961924163600266 | Reward: [1.586610894229775]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 157<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 157,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 157 | Loss: [0.00020093] | LR: (0.12375, 158) | Mean R: 11.961924163600266 | Reward: [1.577958718659545]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 158<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 158,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 158 | Loss: [0.0001948] | LR: (0.12375, 159) | Mean R: 11.961924163600266 | Reward: [1.5693996742981984]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 159<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 159,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 159 | Loss: [0.00018878] | LR: (0.12375, 160) | Mean R: 11.961924163600266 | Reward: [1.5609322790867104]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 160<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 160,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 160 | Loss: [0.00018285] | LR: (0.12375, 161) | Mean R: 11.961924163600266 | Reward: [1.5525550819062683]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 161<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 161,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 161 | Loss: [0.00017702] | LR: (0.12375, 162) | Mean R: 11.961924163600266 | Reward: [1.5442666617857537]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 162<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 162,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 162 | Loss: [0.00017129] | LR: (0.12375, 163) | Mean R: 11.961924163600266 | Reward: [1.5360656271330662]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 163<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 163,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 163 | Loss: [0.00016568] | LR: (0.12375, 164) | Mean R: 11.961924163600266 | Reward: [1.5279506149894555]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 164<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 164,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 164 | Loss: [0.00016017] | LR: (0.12375, 165) | Mean R: 11.961924163600266 | Reward: [1.5199202903060822]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 165<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 165,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 165 | Loss: [0.00015475] | LR: (0.12375, 166) | Mean R: 11.961924163600266 | Reward: [1.5119733452420654]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 166<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 166,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 166 | Loss: [0.00014946] | LR: (0.12375, 167) | Mean R: 11.961924163600266 | Reward: [1.5041084984832835]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 167<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 167,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 167 | Loss: [0.00014424] | LR: (0.12375, 168) | Mean R: 11.961924163600266 | Reward: [1.4963244945812235]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 168<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 168,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 168 | Loss: [0.0001391] | LR: (0.12375, 169) | Mean R: 11.961924163600266 | Reward: [1.4886201033112254]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 169<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 169,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 169 | Loss: [0.0001341] | LR: (0.12375, 170) | Mean R: 11.961924163600266 | Reward: [1.4809941190494822]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 170<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 170,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 170 | Loss: [0.00012923] | LR: (0.12375, 171) | Mean R: 11.961924163600266 | Reward: [1.473445360168073]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 171<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 171,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 171 | Loss: [0.00012449] | LR: (0.12375, 172) | Mean R: 11.961924163600266 | Reward: [1.465972668447625]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 172<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 172,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 172 | Loss: [0.0001199] | LR: (0.12375, 173) | Mean R: 11.961924163600266 | Reward: [1.45857490850684]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 173<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 173,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 173 | Loss: [0.00011544] | LR: (0.12375, 174) | Mean R: 11.961924163600266 | Reward: [1.4512509672483755]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 174<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 174,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 174 | Loss: [0.00011113] | LR: (0.12375, 175) | Mean R: 11.961924163600266 | Reward: [1.443999753320627]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 175<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 175,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 175 | Loss: [0.00010695] | LR: (0.12375, 176) | Mean R: 11.961924163600266 | Reward: [1.4368201965948266]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 176<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 176,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 176 | Loss: [0.00010287] | LR: (0.12375, 177) | Mean R: 11.961924163600266 | Reward: [1.4297112476569094]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 177<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 177,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 177 | Loss: [9.8944605e-05] | LR: (0.12375, 178) | Mean R: 11.961924163600266 | Reward: [1.42267187731378]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 178<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 178,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 178 | Loss: [9.512707e-05] | LR: (0.12375, 179) | Mean R: 11.961924163600266 | Reward: [1.4157010761134696]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 179<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 179,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 179 | Loss: [9.143361e-05] | LR: (0.12375, 180) | Mean R: 11.961924163600266 | Reward: [1.4087978538786619]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 180<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 180,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 180 | Loss: [8.786261e-05] | LR: (0.12375, 181) | Mean R: 11.961924163600266 | Reward: [1.4019612392533052]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 181<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 181,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 181 | Loss: [8.440588e-05] | LR: (0.12375, 182) | Mean R: 11.961924163600266 | Reward: [1.3951902792617545]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 182<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 182,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 182 | Loss: [8.105529e-05] | LR: (0.12375, 183) | Mean R: 11.961924163600266 | Reward: [1.3884840388801418]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 183<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 183,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 183 | Loss: [7.782877e-05] | LR: (0.12375, 184) | Mean R: 11.961924163600266 | Reward: [1.3818416006195218]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 184<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 184,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 184 | Loss: [7.473775e-05] | LR: (0.12375, 185) | Mean R: 11.961924163600266 | Reward: [1.375262064120486]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 185<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 185,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 185 | Loss: [7.171858e-05] | LR: (0.12375, 186) | Mean R: 11.961924163600266 | Reward: [1.3687445457588083]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 186<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 186,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 186 | Loss: [6.8839836e-05] | LR: (0.12375, 187) | Mean R: 11.961924163600266 | Reward: [1.3622881782618599]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 187<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 187,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 187 | Loss: [6.60427e-05] | LR: (0.12375, 188) | Mean R: 11.961924163600266 | Reward: [1.3558921103353647]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 188<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 188,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 188 | Loss: [6.3355e-05] | LR: (0.12375, 189) | Mean R: 11.961924163600266 | Reward: [1.349555506300284]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 189<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 189,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 189 | Loss: [6.0766866e-05] | LR: (0.12375, 190) | Mean R: 11.961924163600266 | Reward: [1.343277545739415]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 190<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 190,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 190 | Loss: [5.826691e-05] | LR: (0.12375, 191) | Mean R: 11.961924163600266 | Reward: [1.3370574231534658]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 191<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 191,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 191 | Loss: [5.5874723e-05] | LR: (0.12375, 192) | Mean R: 11.961924163600266 | Reward: [1.3308943476262804]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 192<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 192,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 192 | Loss: [5.354294e-05] | LR: (0.12375, 193) | Mean R: 11.961924163600266 | Reward: [1.3247875424989815]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 193<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 193,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 193 | Loss: [5.1322175e-05] | LR: (0.12375, 194) | Mean R: 11.961924163600266 | Reward: [1.3187362450526994]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 194<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 194,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 194 | Loss: [4.917491e-05] | LR: (0.12375, 195) | Mean R: 11.961924163600266 | Reward: [1.312739706199661]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 195<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 195,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 195 | Loss: [4.7114165e-05] | LR: (0.12375, 196) | Mean R: 11.961924163600266 | Reward: [1.306797190182401]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 196<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 196,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 196 | Loss: [4.5148136e-05] | LR: (0.12375, 197) | Mean R: 11.961924163600266 | Reward: [1.3009079742807934]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 197<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 197,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 197 | Loss: [4.326702e-05] | LR: (0.12375, 198) | Mean R: 11.961924163600266 | Reward: [1.2950713485267453]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 198<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 198,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 198 | Loss: [4.14561e-05] | LR: (0.12375, 199) | Mean R: 11.961924163600266 | Reward: [1.2892866154262599]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 199<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 199,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 199 | Loss: [3.9734987e-05] | LR: (0.12375, 200) | Mean R: 11.961924163600266 | Reward: [1.283553089688679]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 200<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
************Process:********** 66.66666666666666%
====================Results=======================
--------->Episode: 200,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 200 | Loss: [3.806123e-05] | LR: (0.061875, 201) | Mean R: 11.961924163600266 | Reward: [1.2778700979628788]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 201<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 201,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 201 | Loss: [3.7269263e-05] | LR: (0.061875, 202) | Mean R: 11.961924163600266 | Reward: [1.2722369785802439]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 202<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 202,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 202 | Loss: [3.6457695e-05] | LR: (0.061875, 203) | Mean R: 11.961924163600266 | Reward: [1.2666530813041632]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 203<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 203,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 203 | Loss: [3.5662466e-05] | LR: (0.061875, 204) | Mean R: 11.961924163600266 | Reward: [1.2611177670859401]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 204<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 204,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 204 | Loss: [3.4857425e-05] | LR: (0.061875, 205) | Mean R: 11.961924163600266 | Reward: [1.255630407826807]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 205<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 205,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 205 | Loss: [3.407035e-05] | LR: (0.061875, 206) | Mean R: 11.961924163600266 | Reward: [1.2501903861460217]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 206<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 206,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 206 | Loss: [3.3276734e-05] | LR: (0.061875, 207) | Mean R: 11.961924163600266 | Reward: [1.2447970951547358]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 207<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 207,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 207 | Loss: [3.2489654e-05] | LR: (0.061875, 208) | Mean R: 11.961924163600266 | Reward: [1.2394499382354915]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 208<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 208,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 208 | Loss: [3.169441e-05] | LR: (0.061875, 209) | Mean R: 11.961924163600266 | Reward: [1.2341483288273185]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 209<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 209,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 209 | Loss: [3.0913856e-05] | LR: (0.061875, 210) | Mean R: 11.961924163600266 | Reward: [1.2288916902160398]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 210<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 210,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 210 | Loss: [3.0167608e-05] | LR: (0.061875, 211) | Mean R: 11.961924163600266 | Reward: [1.2236794553298616]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 211<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 211,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 211 | Loss: [2.9403385e-05] | LR: (0.061875, 212) | Mean R: 11.961924163600266 | Reward: [1.2185110665399126]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 212<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 212,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 212 | Loss: [2.865876e-05] | LR: (0.061875, 213) | Mean R: 11.961924163600266 | Reward: [1.2133859754657568]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 213<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 213,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 213 | Loss: [2.794516e-05] | LR: (0.061875, 214) | Mean R: 11.961924163600266 | Reward: [1.208303642785605]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 214<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 214,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 214 | Loss: [2.7256052e-05] | LR: (0.061875, 215) | Mean R: 11.961924163600266 | Reward: [1.203263538051166]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 215<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 215,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 215 | Loss: [2.6548978e-05] | LR: (0.061875, 216) | Mean R: 11.961924163600266 | Reward: [1.1982651395069919]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 216<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 216,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 216 | Loss: [2.5877835e-05] | LR: (0.061875, 217) | Mean R: 11.961924163600266 | Reward: [1.1933079339141823]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 217<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 217,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 217 | Loss: [2.5239353e-05] | LR: (0.061875, 218) | Mean R: 11.961924163600266 | Reward: [1.1883914163783302]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 218<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 218,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 218 | Loss: [2.4592704e-05] | LR: (0.061875, 219) | Mean R: 11.961924163600266 | Reward: [1.1835150901816043]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 219<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 219,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 219 | Loss: [2.3983615e-05] | LR: (0.061875, 220) | Mean R: 11.961924163600266 | Reward: [1.1786784666188321]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 220<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 220,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 220 | Loss: [2.3394114e-05] | LR: (0.061875, 221) | Mean R: 11.961924163600266 | Reward: [1.1738810648374987]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 221<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 221,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 221 | Loss: [2.283074e-05] | LR: (0.061875, 222) | Mean R: 11.961924163600266 | Reward: [1.1691224116814976]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 222<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 222,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 222 | Loss: [2.2278795e-05] | LR: (0.061875, 223) | Mean R: 11.961924163600266 | Reward: [1.1644020415386684]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 223<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 223,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 223 | Loss: [2.1735019e-05] | LR: (0.061875, 224) | Mean R: 11.961924163600266 | Reward: [1.1597194961917907]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 224<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 224,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 224 | Loss: [2.1215737e-05] | LR: (0.061875, 225) | Mean R: 11.961924163600266 | Reward: [1.155074324673178]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 225<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 225,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 225 | Loss: [2.0709516e-05] | LR: (0.061875, 226) | Mean R: 11.961924163600266 | Reward: [1.1504660831226499]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 226<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 226,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 226 | Loss: [2.021309e-05] | LR: (0.061875, 227) | Mean R: 11.961924163600266 | Reward: [1.1458943346487391]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 227<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 227,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 227 | Loss: [1.9759125e-05] | LR: (0.061875, 228) | Mean R: 11.961924163600266 | Reward: [1.1413586491932204]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 228<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 228,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 228 | Loss: [1.9305167e-05] | LR: (0.061875, 229) | Mean R: 11.961924163600266 | Reward: [1.1368586033986912]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 229<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 229,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 229 | Loss: [1.88708e-05] | LR: (0.061875, 230) | Mean R: 11.961924163600266 | Reward: [1.1323937804792568]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 230<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 230,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 230 | Loss: [1.8441326e-05] | LR: (0.061875, 231) | Mean R: 11.961924163600266 | Reward: [1.1279637700941318]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 231<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 231,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 231 | Loss: [1.8028184e-05] | LR: (0.061875, 232) | Mean R: 11.961924163600266 | Reward: [1.123568168224157]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 232<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 232,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 232 | Loss: [1.7634638e-05] | LR: (0.061875, 233) | Mean R: 11.961924163600266 | Reward: [1.1192065770511395]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 233<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 233,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 233 | Loss: [1.727212e-05] | LR: (0.061875, 234) | Mean R: 11.961924163600266 | Reward: [1.1148786048399035]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 234<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 234,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 234 | Loss: [1.6890004e-05] | LR: (0.061875, 235) | Mean R: 11.961924163600266 | Reward: [1.1105838658230276]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 235<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 235,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 235 | Loss: [1.6550344e-05] | LR: (0.061875, 236) | Mean R: 11.961924163600266 | Reward: [1.1063219800881754]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 236<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 236,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 236 | Loss: [1.6197622e-05] | LR: (0.061875, 237) | Mean R: 11.961924163600266 | Reward: [1.1020925734679388]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 237<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 237,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 237 | Loss: [1.5884088e-05] | LR: (0.061875, 238) | Mean R: 11.961924163600266 | Reward: [1.0978952774321975]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 238<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 238,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 238 | Loss: [1.5568923e-05] | LR: (0.061875, 239) | Mean R: 11.961924163600266 | Reward: [1.0937297289827583]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 239<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 239,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 239 | Loss: [1.5266822e-05] | LR: (0.061875, 240) | Mean R: 11.961924163600266 | Reward: [1.0895955705505234]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 240<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 240,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 240 | Loss: [1.4971256e-05] | LR: (0.061875, 241) | Mean R: 11.961924163600266 | Reward: [1.0854924498947387]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 241<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 241,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 241 | Loss: [1.46789525e-05] | LR: (0.061875, 242) | Mean R: 11.961924163600266 | Reward: [1.081420020004595]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 242<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 242,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 242 | Loss: [1.44111455e-05] | LR: (0.061875, 243) | Mean R: 11.961924163600266 | Reward: [1.0773779390029397]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 243<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 243,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 243 | Loss: [1.41498695e-05] | LR: (0.061875, 244) | Mean R: 11.961924163600266 | Reward: [1.0733658700521005]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 244<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 244,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 244 | Loss: [1.3890226e-05] | LR: (0.061875, 245) | Mean R: 11.961924163600266 | Reward: [1.0693834812617808]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 245<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 245,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 245 | Loss: [1.3643646e-05] | LR: (0.061875, 246) | Mean R: 11.961924163600266 | Reward: [1.0654304455989383]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 246<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 246,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 246 | Loss: [1.3416663e-05] | LR: (0.061875, 247) | Mean R: 11.961924163600266 | Reward: [1.0615064407996258]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 247<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 247,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 247 | Loss: [1.3194578e-05] | LR: (0.061875, 248) | Mean R: 11.961924163600266 | Reward: [1.0576111492827494]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 248<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 248,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 248 | Loss: [1.297576e-05] | LR: (0.061875, 249) | Mean R: 11.961924163600266 | Reward: [1.053744258065663]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 249<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 249,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 249 | Loss: [1.274714e-05] | LR: (0.061875, 250) | Mean R: 11.961924163600266 | Reward: [1.0499054586815468]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 250<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
************Process:********** 83.33333333333334%
====================Results=======================
--------->Episode: 250,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 250 | Loss: [1.2564248e-05] | LR: (0.0309375, 251) | Mean R: 11.961924163600266 | Reward: [1.0460944470986444]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 251<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 251,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 251 | Loss: [1.2451572e-05] | LR: (0.0309375, 252) | Mean R: 11.961924163600266 | Reward: [1.0423109236411356]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 252<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 252,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 252 | Loss: [1.2347062e-05] | LR: (0.0309375, 253) | Mean R: 11.961924163600266 | Reward: [1.0385545929116748]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 253<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 253,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 253 | Loss: [1.2278476e-05] | LR: (0.0309375, 254) | Mean R: 11.961924163600266 | Reward: [1.0348251637156807]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 254<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 254,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 254 | Loss: [1.2172333e-05] | LR: (0.0309375, 255) | Mean R: 11.961924163600266 | Reward: [1.0311223489870862]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 255<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 255,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 255 | Loss: [1.2082519e-05] | LR: (0.0309375, 256) | Mean R: 11.961924163600266 | Reward: [1.0274458657157535]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 256<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 256,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 256 | Loss: [1.1981273e-05] | LR: (0.0309375, 257) | Mean R: 11.961924163600266 | Reward: [1.0237954348763338]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 257<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 257,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 257 | Loss: [1.1896357e-05] | LR: (0.0309375, 258) | Mean R: 11.961924163600266 | Reward: [1.0201707813586811]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 258<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 258,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 258 | Loss: [1.180491e-05] | LR: (0.0309375, 259) | Mean R: 11.961924163600266 | Reward: [1.0165716338996464]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 259<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 259,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 259 | Loss: [1.1715096e-05] | LR: (0.0309375, 260) | Mean R: 11.961924163600266 | Reward: [1.0129977250163478]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 260<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 260,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 260 | Loss: [1.1638346e-05] | LR: (0.0309375, 261) | Mean R: 11.961924163600266 | Reward: [1.0094487909407661]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 261<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 261,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 261 | Loss: [1.1564862e-05] | LR: (0.0309375, 262) | Mean R: 11.961924163600266 | Reward: [1.0059245715557168]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 262<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 262,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 262 | Loss: [1.1478314e-05] | LR: (0.0309375, 263) | Mean R: 11.961924163600266 | Reward: [1.0024248103321494]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 263<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 263,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 263 | Loss: [1.1395031e-05] | LR: (0.0309375, 264) | Mean R: 11.961924163600266 | Reward: [0.9989492542676821]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 264<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 264,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 264 | Loss: [1.1308482e-05] | LR: (0.0309375, 265) | Mean R: 11.961924163600266 | Reward: [0.9954976538264511]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 265<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 265,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 265 | Loss: [1.1234996e-05] | LR: (0.0309375, 266) | Mean R: 11.961924163600266 | Reward: [0.9920697628801207]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 266<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 266,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 266 | Loss: [1.115498e-05] | LR: (0.0309375, 267) | Mean R: 11.961924163600266 | Reward: [0.9886653386501347]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 267<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 267,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 267 | Loss: [1.1088027e-05] | LR: (0.0309375, 268) | Mean R: 11.961924163600266 | Reward: [0.9852841416511229]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 268<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 268,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 268 | Loss: [1.1022707e-05] | LR: (0.0309375, 269) | Mean R: 11.961924163600266 | Reward: [0.9819259356354415]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 269<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 269,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 269 | Loss: [1.0941058e-05] | LR: (0.0309375, 270) | Mean R: 11.961924163600266 | Reward: [0.9785904875388045]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 270<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 270,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 270 | Loss: [1.0865941e-05] | LR: (0.0309375, 271) | Mean R: 11.961924163600266 | Reward: [0.9752775674270477]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 271<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 271,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 271 | Loss: [1.079409e-05] | LR: (0.0309375, 272) | Mean R: 11.961924163600266 | Reward: [0.9719869484439307]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 272<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 272,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 272 | Loss: [1.0720605e-05] | LR: (0.0309375, 273) | Mean R: 11.961924163600266 | Reward: [0.9687184067599617]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 273<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 273,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 273 | Loss: [1.0658551e-05] | LR: (0.0309375, 274) | Mean R: 11.961924163600266 | Reward: [0.9654717215222632]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 274<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 274,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 274 | Loss: [1.0604663e-05] | LR: (0.0309375, 275) | Mean R: 11.961924163600266 | Reward: [0.9622466748054066]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 275<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 275,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 275 | Loss: [1.0537711e-05] | LR: (0.0309375, 276) | Mean R: 11.961924163600266 | Reward: [0.9590430515632402]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 276<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 276,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 276 | Loss: [1.0472393e-05] | LR: (0.0309375, 277) | Mean R: 11.961924163600266 | Reward: [0.9558606395816067]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 277<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 277,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 277 | Loss: [1.0395644e-05] | LR: (0.0309375, 278) | Mean R: 11.961924163600266 | Reward: [0.9526992294320671]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 278<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 278,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 278 | Loss: [1.0336856e-05] | LR: (0.0309375, 279) | Mean R: 11.961924163600266 | Reward: [0.9495586144264614]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 279<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 279,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 279 | Loss: [1.0278069e-05] | LR: (0.0309375, 280) | Mean R: 11.961924163600266 | Reward: [0.9464385905723773]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 280<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 280,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 280 | Loss: [1.020785e-05] | LR: (0.0309375, 281) | Mean R: 11.961924163600266 | Reward: [0.9433389565294732]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 281<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 281,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 281 | Loss: [1.0153961e-05] | LR: (0.0309375, 282) | Mean R: 11.961924163600266 | Reward: [0.940259513566648]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 282<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 282,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 282 | Loss: [1.0091907e-05] | LR: (0.0309375, 283) | Mean R: 11.961924163600266 | Reward: [0.9372000655200594]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 283<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 283,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 283 | Loss: [1.0036385e-05] | LR: (0.0309375, 284) | Mean R: 11.961924163600266 | Reward: [0.9341604187518939]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 284<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 284,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 284 | Loss: [9.977597e-06] | LR: (0.0309375, 285) | Mean R: 11.961924163600266 | Reward: [0.9311403821099962]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 285<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 285,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 285 | Loss: [9.9139115e-06] | LR: (0.0309375, 286) | Mean R: 11.961924163600266 | Reward: [0.9281397668881723]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 286<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 286,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 286 | Loss: [9.85349e-06] | LR: (0.0309375, 287) | Mean R: 11.961924163600266 | Reward: [0.9251583867873663]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 287<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 287,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 287 | Loss: [9.807767e-06] | LR: (0.0309375, 288) | Mean R: 11.961924163600266 | Reward: [0.922196057877473]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 288<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 288,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 288 | Loss: [9.75551e-06] | LR: (0.0309375, 289) | Mean R: 11.961924163600266 | Reward: [0.9192525985599325]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 289<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 289,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 289 | Loss: [9.685294e-06] | LR: (0.0309375, 290) | Mean R: 11.961924163600266 | Reward: [0.9163278295309798]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 290<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 290,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 290 | Loss: [9.6395715e-06] | LR: (0.0309375, 291) | Mean R: 11.961924163600266 | Reward: [0.9134215737456479]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 291<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 291,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 291 | Loss: [9.592214e-06] | LR: (0.0309375, 292) | Mean R: 11.961924163600266 | Reward: [0.9105336563823805]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 292<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 292,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 292 | Loss: [9.548123e-06] | LR: (0.0309375, 293) | Mean R: 11.961924163600266 | Reward: [0.9076639048083557]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 293<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 293,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 293 | Loss: [9.490969e-06] | LR: (0.0309375, 294) | Mean R: 11.961924163600266 | Reward: [0.9048121485454104]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 294<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 294,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 294 | Loss: [9.448511e-06] | LR: (0.0309375, 295) | Mean R: 11.961924163600266 | Reward: [0.9019782192366463]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 295<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 295,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 295 | Loss: [9.391357e-06] | LR: (0.0309375, 296) | Mean R: 11.961924163600266 | Reward: [0.8991619506136104]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 296<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 296,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 296 | Loss: [9.335837e-06] | LR: (0.0309375, 297) | Mean R: 11.961924163600266 | Reward: [0.8963631784641066]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 297<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 297,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 297 | Loss: [9.303177e-06] | LR: (0.0309375, 298) | Mean R: 11.961924163600266 | Reward: [0.8935817406005722]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 298<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 298,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
=-=-=-=-=-=>Episode: 298 | Loss: [9.249287e-06] | LR: (0.0309375, 299) | Mean R: 11.961924163600266 | Reward: [0.8908174768290937]<=-=-=-=-=
=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 299<=-=-==-=-==-=-==-=-==-=-==-=-==-=-==-=-=
====================Results=======================
--------->Episode: 299,sparsity_level:[2, 3, 4]
--------->Accuracy reward: 0.8210000726911757,time reward:11.14092409090909
--------->Reward: 11.961924163600266
==========================================================================================
--------->mask_dict_set:[{'fc1.weight': [tensor([[0, 1, 0, 1, 1, 0, 0, 1, 0, 1],
        [1, 0, 1, 1, 0, 0, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 0, 1, 0, 1, 1],
        [1, 0, 1, 0, 0, 1, 1, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 0, 1, 1, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 1, 1],
        [0, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 0, 0, 0, 0, 1, 1, 1, 0],
        [0, 1, 0, 0, 0, 1, 1, 0, 0, 1]], dtype=torch.int32), tensor([[0, 1, 1, 1, 0, 1, 0, 1, 1, 1],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 0, 0, 1, 0, 0],
        [0, 1, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 0, 1, 1, 0, 1],
        [0, 1, 0, 1, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 1, 1, 1, 1, 1, 1, 0, 0]], dtype=torch.int32), tensor([[0, 1, 1, 1, 0, 1, 0, 1, 1, 1],
        [1, 0, 1, 0, 0, 0, 0, 1, 1, 0],
        [0, 1, 1, 1, 1, 0, 1, 1, 1, 1],
        [1, 0, 0, 1, 1, 1, 0, 0, 1, 1],
        [0, 0, 1, 1, 0, 0, 1, 1, 0, 1],
        [0, 0, 0, 1, 1, 1, 1, 0, 1, 1],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 1, 1, 1, 1, 0, 1, 0]], dtype=torch.int32), tensor([[0, 1, 1, 1, 0, 1, 0, 1, 1, 1],
        [1, 0, 1, 0, 0, 0, 0, 1, 1, 0],
        [0, 1, 1, 1, 1, 0, 1, 1, 1, 1],
        [1, 0, 0, 1, 1, 1, 0, 0, 1, 1],
        [0, 0, 1, 1, 0, 0, 1, 1, 0, 1],
        [0, 0, 0, 1, 1, 1, 1, 0, 1, 1],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 1, 1, 1, 1, 0, 1, 0]], dtype=torch.int32)], 'fc2.weight': [tensor([[1, 1, 0, 0, 0, 0, 0, 1, 1, 1],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 1, 1, 1],
        [1, 1, 1, 1, 0, 0, 1, 1, 1, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 1],
        [0, 1, 1, 0, 1, 1, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 1, 1, 0]], dtype=torch.int32), tensor([[1, 1, 0, 0, 0, 0, 0, 1, 1, 1],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 1, 1, 1],
        [1, 1, 1, 1, 0, 0, 1, 1, 1, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 1],
        [0, 1, 1, 0, 1, 1, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 1, 1, 0]], dtype=torch.int32), tensor([[0, 1, 0, 1, 1, 1, 0, 0, 0, 1],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 1],
        [0, 1, 1, 0, 1, 1, 0, 0, 1, 1],
        [1, 0, 0, 0, 0, 0, 1, 1, 1, 1],
        [0, 1, 1, 1, 0, 1, 0, 1, 1, 0],
        [1, 1, 0, 0, 0, 1, 1, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 0, 0, 1, 1],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 1, 1, 0]], dtype=torch.int32), tensor([[0, 1, 0, 0, 0, 1, 1, 0, 1, 1],
        [0, 1, 0, 0, 1, 0, 0, 1, 0, 1],
        [1, 1, 0, 0, 0, 0, 1, 1, 1, 1],
        [1, 0, 1, 1, 1, 0, 0, 1, 1, 0],
        [1, 0, 0, 1, 0, 1, 1, 0, 0, 1],
        [1, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 1, 1, 1, 1, 0, 1, 1, 1, 0],
        [1, 0, 1, 1, 0, 0, 0, 1, 1, 0]], dtype=torch.int32)], 'fc3.weight': [tensor([[0, 0, 1, 1, 1, 0, 0, 1, 1, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 0, 1, 1, 1, 0, 0, 0, 1, 1],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 1, 0, 0, 1, 0, 1, 1],
        [0, 1, 0, 1, 1, 1, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 0, 1, 1, 1, 1],
        [0, 0, 1, 1, 1, 1, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 1, 1],
        [0, 1, 0, 1, 0, 0, 1, 1, 1, 0]], dtype=torch.int32), tensor([[0, 1, 0, 0, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 1, 0, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 1, 0, 1, 1, 1],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 1],
        [0, 1, 1, 1, 1, 0, 0, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 0, 1, 0],
        [1, 0, 0, 1, 0, 1, 1, 1, 0, 1],
        [0, 0, 1, 1, 0, 0, 1, 1, 1, 0]], dtype=torch.int32), tensor([[1, 0, 1, 1, 1, 0, 0, 1, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 1, 1, 0, 0, 1, 0, 1, 0, 1],
        [1, 1, 1, 1, 0, 0, 0, 1, 0, 1],
        [0, 1, 0, 1, 0, 0, 0, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 1],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 1, 1, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 1, 0, 0, 0, 1],
        [0, 1, 0, 1, 1, 1, 1, 1, 1, 0]], dtype=torch.int32), tensor([[1, 0, 1, 1, 1, 0, 0, 1, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 1, 1, 0, 0, 1, 0, 1, 0, 1],
        [1, 1, 1, 1, 0, 0, 0, 1, 0, 1],
        [0, 1, 0, 1, 0, 0, 0, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 1],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 1, 1, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 1, 0, 0, 0, 1],
        [0, 1, 0, 1, 1, 1, 1, 1, 1, 0]], dtype=torch.int32)]}, {'fc1.weight': [tensor([[0, 0, 1, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 1, 1, 0, 1],
        [0, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 1],
        [0, 0, 0, 1, 1, 1, 0, 0, 1, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 1, 1, 1, 0, 1, 0]], dtype=torch.int32), tensor([[0, 0, 1, 0, 0, 0, 1, 0, 1, 1],
        [0, 0, 1, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 1, 1, 0],
        [1, 0, 0, 0, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 1, 1],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 0, 1, 0]], dtype=torch.int32), tensor([[0, 0, 1, 0, 0, 0, 1, 0, 1, 1],
        [0, 0, 1, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 1, 1, 0],
        [1, 0, 0, 0, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 1, 1],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 0, 1, 0]], dtype=torch.int32), tensor([[0, 0, 1, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 1, 1, 0, 1],
        [0, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 1],
        [0, 0, 0, 1, 1, 1, 0, 0, 1, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 1, 1, 1, 0, 1, 0]], dtype=torch.int32)], 'fc2.weight': [tensor([[1, 1, 0, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 1, 0, 0, 1, 0, 1, 0],
        [1, 0, 1, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 0]], dtype=torch.int32), tensor([[1, 1, 0, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 1, 0, 0, 1, 0, 1, 0],
        [1, 0, 1, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 0]], dtype=torch.int32), tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 1, 1, 1],
        [0, 0, 0, 1, 1, 0, 0, 1, 0, 0],
        [1, 0, 0, 1, 0, 0, 1, 0, 0, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 1, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0]], dtype=torch.int32), tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 1, 1, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0]], dtype=torch.int32)], 'fc3.weight': [tensor([[0, 0, 1, 0, 1, 1, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 1],
        [0, 0, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 1, 0, 0, 0, 1],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 1, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 1, 1, 0, 0, 0, 1],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 1, 0, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0, 0, 1, 1, 0, 1, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 1, 0, 0],
        [1, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [0, 0, 1, 1, 0, 0, 1, 1, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0, 0, 1, 0, 0, 1, 1, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 1, 0, 1, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 1, 1, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 1, 0, 0, 0]], dtype=torch.int32)]}, {'fc1.weight': [tensor([[0, 0, 1, 0, 0, 0, 1, 0, 1, 1],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=torch.int32), tensor([[0, 0, 1, 0, 0, 1, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0]], dtype=torch.int32), tensor([[0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 1, 1, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 1, 1, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 1, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 1, 0, 0, 0]], dtype=torch.int32)], 'fc2.weight': [tensor([[1, 1, 0, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 1, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0]], dtype=torch.int32), tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 1],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 1, 0, 1, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=torch.int32), tensor([[0, 1, 0, 0, 1, 0, 0, 1, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 1, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1]], dtype=torch.int32)], 'fc3.weight': [tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0, 0, 1, 0, 0, 1, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 1, 1, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 1, 0, 0, 0]], dtype=torch.int32)]}]/home/shz15022/RT3_opensource/RL/Pruning/mnist_pattern_pruning_CPU.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  tensor_pattern = torch.tensor(temp_pattern)
/home/shz15022/anaconda3/envs/RT3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  "please use `get_last_lr()`.", UserWarning)

=-=-=-=-=-=>Episode: 299 | Loss: [9.192134e-06] | LR: (0.0309375, 300) | Mean R: 11.961924163600266 | Reward: [0.8880702289189006]<=-=-=-=-=
reward history: [7.767390569026445, 7.075259545454545, 5.418202272727273, 9.363491800486246, 8.737640909090908, 11.027589411887737, 5.426285, 11.936397978036283, 5.436971818181818, 11.967569189084756, 7.765547675801287, 3.4124122727272725, 8.77324409090909, 4.715878181818182, 7.073966818181818, 7.070158181818181, 6.339631818181818, 11.96275481016525, 7.787882932140466, 6.337805909090909, 8.743406818181818, 11.986077641137634, 5.423649999999999, 11.982137183637521, 11.936746969670766, 4.7141709090909085, 7.774962795844029, 4.717523636363636, 4.718680909090909, 11.437105216987685, 4.697338636363636, 5.440152272727272, 5.438591363636363, 9.373552068362766, 8.731876818181817, 11.93496552099825, 8.738199545454545, 7.066200454545454, 11.940435536415793, 11.076259904977046, 5.435634545454545, 3.4101786363636357, 11.94601768173449, 11.396148950798727, 11.913824417529636, 6.337079545454545, 11.961829829120056, 11.928929107280693, 11.935350500573051, 11.928342018864177, 11.972836201644279, 11.942875705017705, 11.930548255688445, 11.971024132182574, 11.871374048894246, 11.971745821776727, 11.476095866202153, 11.963792730636596, 11.930523911723897, 11.960373389918008, 11.907138838807692, 11.994209970767088, 11.954077438163372, 11.923632710159763, 11.916510754793725, 11.887843569752086, 11.892614544633036, 11.903169864948927, 11.362147359591397, 11.938507475476504, 11.870174723884505, 11.963841436327538, 11.949285319799557, 11.954537700689295, 11.967975190920589, 11.967975190920589, 11.917230455499514, 11.911897164339779, 11.911897164339779, 11.887564561520971, 11.912675336091013, 11.398464797688494, 11.398464797688494, 11.948910195383398, 11.435132250606651, 11.948910195383398, 11.939230233584317, 11.930452456230395, 11.930452456230395, 11.383935993014248, 11.909491153324781, 11.909491153324781, 11.888158650959669, 11.888158650959669, 11.952825054624151, 11.931380627981628, 11.931380627981628, 11.957380174032654, 11.957380174032654, 11.957380174032654, 11.957380174032654, 11.957380174032654, 11.957380174032654, 11.957380174032654, 11.957380174032654, 11.957380174032654, 11.957380174032654, 11.943583106332643, 11.909479361210062, 11.909479361210062, 11.909479361210062, 11.943368575407856, 11.943368575407856, 11.943368575407856, 11.943368575407856, 11.943368575407856, 11.943368575407856, 11.943368575407856, 11.943368575407856, 11.943368575407856, 11.943368575407856, 11.943368575407856, 11.943368575407856, 11.943368575407856, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266, 11.961924163600266]
